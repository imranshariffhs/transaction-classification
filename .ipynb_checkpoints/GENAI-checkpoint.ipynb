{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlsSMC05AXRB"
   },
   "source": [
    "# **Install All Packages**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "qsTyXV4zAg6N",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install --upgrade google-generativeai\n",
    "# !pip install langchain-google-genai --quiet\n",
    "# !pip install langchain-community\n",
    "# !pip install typesense\n",
    "# !pip install -U langchain langchain-google-genai langchain-community\n",
    "# !pip install -qU langchain_postgres\n",
    "# !pip install -qU duckduckgo-search langchain-community\n",
    "# !pip install --upgrade --quiet  lark pgvector psycopg2-binary\n",
    "# !pip install pandas\n",
    "# !pip install openpyxl\n",
    "# !pip install google-generativeai\n",
    "# !pip install python-dotenv\n",
    "# !pip install qdrant-client\n",
    "# !pip install python-dotenv\n",
    "# !pip install pgvector psycopg2-binary\n",
    "# !pip install timescale-vector\n",
    "# !pip install google-ai-generativelanguage==0.4.0\n",
    "\n",
    "# #get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install --upgrade torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y google-generativeai google-ai-generativelanguage langchain-google-genai\n",
    "# !pip install google-generativeai==0.3.2\n",
    "# !pip install google-ai-generativelanguage==0.3.1\n",
    "# !pip install langchain-google-genai==0.0.6\n",
    "\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension --sys-prefix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninstall current torch\n",
    "# !pip uninstall torch -y\n",
    "\n",
    "# # Install PyTorch 2.5.1 and compatible versions\n",
    "# !pip install torch==2.5.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iblagocDBG0M"
   },
   "source": [
    "# **Import all necessary packages and modules**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "x8rHieyd-Pc4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import timedelta\n",
    "\n",
    "import sqlite3  # Added for SQLite fallback\n",
    "import psycopg2\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_api_call(func, max_retries=3, delay=1):\n",
    "    \"\"\"Safe API call with retries\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(delay * (2 ** attempt))\n",
    "            else:\n",
    "                return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dMFC47yA4Pa"
   },
   "source": [
    "# **Set your Google API key** *local we using .env*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "4Ol80x9T_KYF"
   },
   "outputs": [],
   "source": [
    "# Set your Google API key (if not already in environment)\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAQZcS0H6s3IBrIMyGAw3_tnRaQanFzpBk\"\n",
    "    API_KEY = \"AIzaSyAQZcS0H6s3IBrIMyGAw3_tnRaQanFzpBk\" #\"AIzaSyBz0ERhDhWnh_KO-GWeNqe7S-j8SBmt22k\" #AIzaSyC9H2klWHV9YCHooUPukXvWGztUbacuQMM AIzaSyBazIeacy7eubIZ6J-h6H5DBo2EoJS1gU4\n",
    "    SEARCH_ENGINE_ID = \"94f57e531fc964fd5\"  # Your CSE ID from the HTML code a0fcf4aeb345944e6 f3df791282bb246c7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNMBqaQqBf7W"
   },
   "source": [
    "# **Set your LangSmith key** *local we using .env*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "nDOd-bfCAzND"
   },
   "outputs": [],
   "source": [
    "# Set your LangSmith API key (optional, for tracing)\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU3qScBKCD_i"
   },
   "source": [
    "# **Create the ChatGoogleGenerativeAI instance**  I using gemini-2.0-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "VhA59KymAvGK"
   },
   "outputs": [],
   "source": [
    "# # Create the ChatGoogleGenerativeAI instance\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-2.0-flash\",\n",
    "#     temperature=0,\n",
    "#     max_tokens=None,\n",
    "#     timeout=None,\n",
    "#     max_retries=2,\n",
    "#     # other params...\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfm7nZJUCWz4"
   },
   "source": [
    "#  **Calling Our Models or LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "WLBQShlAAyIf"
   },
   "outputs": [],
   "source": [
    "# # Make a call to the Gemini model\n",
    "# response = llm.invoke(\"Can you explain the concept of 'few-shot learning' in LLMs and its advantages?\")\n",
    "# # Access the generated text\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading KEY from the environment variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set your Google API key (replace with your actual key or use environment variable)\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAQZcS0H6s3IBrIMyGAw3_tnRaQanFzpBk\" # \"AIzaSyBvYAw9t-xndPUNBJ4EKhWv6_l_nJp6_yo\"\n",
    "\n",
    "# Set your LangSmith key (optional, for tracing)\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Interactive setup for database URL if not set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_database_url():\n",
    "    \"\"\"Interactive setup for database URL if not set\"\"\"\n",
    "    if os.getenv(\"DATABASE_URL\"):\n",
    "        return os.getenv(\"DATABASE_URL\")\n",
    "    \n",
    "    print(\"\\nüîß Database Setup Required\")\n",
    "    print(\"   PostgreSQL with vector_user (recommended)\")\n",
    "\n",
    "    print(\"\\nUsing vector_user credentials...\")\n",
    "    host = 'localhost'\n",
    "    port = 5432 \n",
    "    database = 'vector_db' \n",
    "    username = \"vector_user\"\n",
    "    password = \"SecurePassword123!\"\n",
    "        \n",
    "    database_url = f\"postgresql://{username}:{password}@{host}:{port}/{database}\"\n",
    "    os.environ[\"DATABASE_URL\"] = database_url\n",
    "    print(f\"‚úÖ Using: postgresql://vector_user:***@{host}:{port}/{database}\")\n",
    "    return database_url\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Rate Limiting for API and **Process Transactions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RateLimitHandler:\n",
    "    \"\"\"Handle rate limiting for API calls\"\"\"\n",
    "    def __init__(self, requests_per_minute=8):  # Even more conservative\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.request_times = []\n",
    "        self.last_rate_limit_time = 0\n",
    "    \n",
    "    def wait_if_needed(self):\n",
    "        \"\"\"Wait if we're approaching rate limits\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # If we recently hit a rate limit, wait extra time\n",
    "        if current_time - self.last_rate_limit_time < 60:\n",
    "            extra_wait = 60 - (current_time - self.last_rate_limit_time)\n",
    "            if extra_wait > 0:\n",
    "                logger.info(f\"Recent rate limit detected. Extra wait: {extra_wait:.1f} seconds...\")\n",
    "                time.sleep(extra_wait)\n",
    "                current_time = time.time()\n",
    "        \n",
    "        # Remove requests older than 1 minute\n",
    "        self.request_times = [t for t in self.request_times if current_time - t < 60]\n",
    "        \n",
    "        if len(self.request_times) >= self.requests_per_minute:\n",
    "            # Wait until the oldest request is more than 1 minute old\n",
    "            sleep_time = 60 - (current_time - self.request_times[0]) + 2  # +2 seconds buffer\n",
    "            if sleep_time > 0:\n",
    "                logger.info(f\"Proactive rate limit wait: {sleep_time:.1f} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "                # Clean up old requests again\n",
    "                current_time = time.time()\n",
    "                self.request_times = [t for t in self.request_times if current_time - t < 60]\n",
    "        \n",
    "        self.request_times.append(current_time)\n",
    "    \n",
    "    def record_rate_limit_hit(self):\n",
    "        \"\"\"Record when we hit a rate limit\"\"\"\n",
    "        self.last_rate_limit_time = time.time()\n",
    "        # Clear recent requests to reset our tracking\n",
    "        self.request_times = []\n",
    "\n",
    "def make_llm_request(llm, message, rate_limiter, max_retries=3):\n",
    "    \"\"\"Make LLM request with custom rate limit handling\"\"\"\n",
    "    rate_limiter.wait_if_needed()\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return llm.invoke(message)\n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            \n",
    "            # Check if it's a rate limit error\n",
    "            if \"ResourceExhausted\" in error_str or \"429\" in error_str or \"quota\" in error_str.lower():\n",
    "                wait_time = 30  # Wait 30 seconds for rate limit errors\n",
    "                print(f\"‚è∞ Rate limit hit (attempt {attempt + 1}/{max_retries}). Waiting {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                \n",
    "                if attempt < max_retries - 1:  # Don't wait after the last attempt\n",
    "                    continue\n",
    "            else:\n",
    "                # For other errors, wait shorter time\n",
    "                wait_time = 5 * (attempt + 1)  # 5, 10, 15 seconds\n",
    "                print(f\"‚ö†Ô∏è  API error (attempt {attempt + 1}/{max_retries}): {error_str}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"   Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "            \n",
    "            # If we reach here, it's the final attempt or a non-retryable error\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Process bank transactions with rate limiting and error handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_bank_transactions(limit: int = 10,batch_size: int = 50000):\n",
    "    \"\"\"Process bank transactions with rate limiting and error handling\"\"\"\n",
    "    #print(f\"\\nüîÑ Processing bank transactions (limit: {limit})...\")\n",
    "    \n",
    "    # Initialize rate limiter\n",
    "    rate_limiter = RateLimitHandler(requests_per_minute=10)\n",
    "    \n",
    "    try:\n",
    "        filename = \"bank_transactions_with_vendor_100.csv\"\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"‚ö†Ô∏è  File {filename} not found.\")\n",
    "            return []\n",
    "        \n",
    "        df = pd.read_csv(filename)\n",
    "        print(f\"‚úÖ Loaded {len(df)} transactions\")\n",
    "        \n",
    "        # Limit the dataframe if specified\n",
    "        if limit > 0:\n",
    "            df = df.head(limit)\n",
    "            print(f\"üìä Processing first {len(df)} transactions\")\n",
    "        \n",
    "        # Create LLM instance\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            temperature=0,\n",
    "            convert_system_message_to_human=True\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "        failed_transactions = []\n",
    "        \n",
    "        # Process in batches to save progress\n",
    "        for batch_start in range(0, len(df), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(df))\n",
    "            batch_df = df.iloc[batch_start:batch_end]\n",
    "            \n",
    "            print(f\"\\nüì¶ Processing batch {batch_start//batch_size + 1}: transactions {batch_start+1}-{batch_end}\")\n",
    "            \n",
    "            for i, (idx, row) in enumerate(batch_df.iterrows()):\n",
    "                try:\n",
    "                    print(f\"üîÑ Processing transaction {idx+1}: {row['Description'][:50]}...\")\n",
    "                    \n",
    "                    # Single message prompt\n",
    "                    message = (\n",
    "                        f\"Categorize this financial transaction into exactly one of these categories: \"\n",
    "                        f\"Education, Health, Groceries, Transportation, Utilities, Entertainment, Shopping, Other\\n\\n\"\n",
    "                        f\"Transaction: {row['Description']} at {row['Vendor']} for ${row['Amount']}\\n\\n\"\n",
    "                        f\"Respond with only the category name, nothing else.\"\n",
    "                    )\n",
    "                    \n",
    "                    # Make request with rate limiting and retry\n",
    "                    response = make_llm_request(llm, message, rate_limiter)\n",
    "                    category = response.content.strip().replace(\"Category:\", \"\").strip()\n",
    "                    \n",
    "                    # Ensure it's a valid category\n",
    "                    valid_categories = ['Education', 'Health', 'Groceries', 'Transportation', \n",
    "                                     'Utilities', 'Entertainment', 'Shopping', 'Other']\n",
    "                    \n",
    "                    if category not in valid_categories:\n",
    "                        for valid_cat in valid_categories:\n",
    "                            if valid_cat.lower() in category.lower():\n",
    "                                category = valid_cat\n",
    "                                break\n",
    "                        else:\n",
    "                            category = 'Other'\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"transaction_id\": idx + 1,\n",
    "                        \"description\": row[\"Description\"],\n",
    "                        \"vendor\": row[\"Vendor\"],\n",
    "                        \"amount\": row[\"Amount\"],\n",
    "                        \"category\": category\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"‚úÖ Category: {category}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = str(e)\n",
    "                    print(f\"‚ùå Error processing transaction {idx+1}: {error_msg}\")\n",
    "                    \n",
    "                    # Handle specific rate limit errors\n",
    "                    if \"ResourceExhausted\" in error_msg or \"429\" in error_msg or \"quota\" in error_msg.lower():\n",
    "                        print(\"‚è∞ Rate limit detected. Recording for future avoidance...\")\n",
    "                        rate_limiter.record_rate_limit_hit()\n",
    "                        # Add to failed list for potential retry\n",
    "                        failed_transactions.append((idx, row))\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"transaction_id\": idx + 1,\n",
    "                        \"description\": row[\"Description\"],\n",
    "                        \"vendor\": row[\"Vendor\"],\n",
    "                        \"amount\": row[\"Amount\"],\n",
    "                        \"category\": \"Error\"\n",
    "                    })\n",
    "            \n",
    "        \n",
    "        # Retry failed transactions once\n",
    "        if failed_transactions:\n",
    "            print(f\"\\nüîÑ Retrying {len(failed_transactions)} failed transactions...\")\n",
    "            print(\"‚è∞ Waiting 60 seconds before retry to ensure rate limits are reset...\")\n",
    "            time.sleep(60)  # Wait before retrying\n",
    "            \n",
    "            for idx, row in failed_transactions:\n",
    "                try:\n",
    "                    message = (\n",
    "                        f\"Categorize this financial transaction into exactly one of these categories: \"\n",
    "                        f\"Education, Health, Groceries, Transportation, Utilities, Entertainment, Shopping, Other\\n\\n\"\n",
    "                        f\"Transaction: {row['Description']} at {row['Vendor']} for ${row['Amount']}\\n\\n\"\n",
    "                        f\"Respond with only the category name, nothing else.\"\n",
    "                    )\n",
    "                    \n",
    "                    response = make_llm_request(llm, message, rate_limiter)\n",
    "                    category = response.content.strip().replace(\"Category:\", \"\").strip()\n",
    "                    \n",
    "                    # Update the result\n",
    "                    for result in results:\n",
    "                        if result[\"transaction_id\"] == idx + 1:\n",
    "                            result[\"category\"] = category if category in valid_categories else 'Other'\n",
    "                            break\n",
    "                    \n",
    "                    print(f\"‚úÖ Retry successful for transaction {idx+1}: {category}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Retry failed for transaction {idx+1}: {e}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Processing complete! Processed {len(results)} transactions\")\n",
    "        print(results)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Transaction processing failed: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Simple in-memory vector store for testing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Optional\n",
    "import math\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"Simple in-memory vector store for testing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "        \n",
    "        # Configure device and load SentenceTransformer model\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"üîß Using device: {self.device}\")\n",
    "        \n",
    "        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=self.device)\n",
    "        \n",
    "        print(\"‚úÖ SimpleVectorStore initialized with SentenceTransformer\")\n",
    "    \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding using SentenceTransformer\"\"\"\n",
    "        try:\n",
    "            # Generate embedding and convert to list\n",
    "            embedding = self.model.encode(text, convert_to_tensor=False)\n",
    "            return embedding.tolist()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating embedding: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def add_document(self, text: str, metadata: Optional[Dict] = None):\n",
    "        \"\"\"Add document to the store\"\"\"\n",
    "        embedding = self.get_embedding(text)\n",
    "        if embedding:\n",
    "            self.documents.append({\"text\": text, \"metadata\": metadata or {}})\n",
    "            self.embeddings.append(embedding)\n",
    "            print(f\"‚úÖ Added document: '{text[:50]}...'\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def search_similar(self, query: str, limit: int = 3):\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        query_embedding = self.get_embedding(query)\n",
    "        if not query_embedding:\n",
    "            return []\n",
    "        \n",
    "        def cosine_similarity(a, b):\n",
    "            dot_product = sum(x * y for x, y in zip(a, b))\n",
    "            magnitude_a = math.sqrt(sum(x * x for x in a))\n",
    "            magnitude_b = math.sqrt(sum(x * x for x in b))\n",
    "            return dot_product / (magnitude_a * magnitude_b) if magnitude_a and magnitude_b else 0\n",
    "        \n",
    "        similarities = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "            similarity = cosine_similarity(query_embedding, embedding)\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity and get top results\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        results = []\n",
    "        \n",
    "        for i, similarity in similarities[:limit]:\n",
    "            results.append({\n",
    "                \"content\": self.documents[i][\"text\"],\n",
    "                \"metadata\": self.documents[i][\"metadata\"],\n",
    "                \"similarity\": similarity\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vector store with PostgreSQL database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Union, List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import math\n",
    "from sqlalchemy import create_engine,text\n",
    "\n",
    "\n",
    "\n",
    "class DatabaseVectorStore:\n",
    "    \"\"\"Vector store with PostgreSQL database (requires pgvector extension)\"\"\"\n",
    "    \n",
    "    def __init__(self, database_url: Optional[str] = None):\n",
    "        \"\"\"Initialize with database connection\"\"\"\n",
    "        self.engine = create_engine(database_url)\n",
    "        self.database_url = database_url or os.getenv(\"DATABASE_URL\", \"postgresql://vector_user:SecurePassword123!@localhost:5432/vector_db\")\n",
    "        self.embedding_dimensions = 1536  # Required by your schema\n",
    "        \n",
    "        # Configure device and load SentenceTransformer model\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"üîß Using device: {self.device}\")\n",
    "        \n",
    "        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=self.device)\n",
    "        self.model_dimensions = 384  # all-MiniLM-L6-v2 dimensions\n",
    "        \n",
    "        # Test database connection\n",
    "        if not self._test_connection():\n",
    "            raise ConnectionError(\"Failed to connect to database\")\n",
    "        \n",
    "        print(\"‚úÖ DatabaseVectorStore initialized with SentenceTransformer\")\n",
    "    \n",
    "    def get_embedding(self, text: Union[str, dict]) -> List[float]:\n",
    "        \"\"\"Generate embedding using SentenceTransformer with dimension padding\"\"\"\n",
    "        # Handle different input types\n",
    "        if isinstance(text, dict):\n",
    "            # Convert dict to string representation\n",
    "            text_content = str(text)\n",
    "        elif isinstance(text, str):\n",
    "            text_content = text\n",
    "        else:\n",
    "            print(f\"‚ùå Unsupported input type: {type(text)}\")\n",
    "            return []\n",
    "        \n",
    "        if not text_content or not text_content.strip():\n",
    "            return []\n",
    "        \n",
    "        text_content = text_content.replace(\"\\n\", \" \").strip()\n",
    "        \n",
    "        def _generate_embedding():\n",
    "            # Generate embedding and convert to list\n",
    "            embedding = self.model.encode(text_content, convert_to_tensor=False)\n",
    "            return embedding.tolist()\n",
    "        \n",
    "        base_embedding = safe_api_call(_generate_embedding)\n",
    "        if base_embedding:\n",
    "            # Pad the embedding to match required dimensions\n",
    "            if len(base_embedding) < self.embedding_dimensions:\n",
    "                # Calculate how many times to repeat the base embedding\n",
    "                repeat_times = self.embedding_dimensions // len(base_embedding)\n",
    "                remainder = self.embedding_dimensions % len(base_embedding)\n",
    "                \n",
    "                # Create padded embedding\n",
    "                padded_embedding = base_embedding * repeat_times\n",
    "                if remainder:\n",
    "                    padded_embedding.extend(base_embedding[:remainder])\n",
    "                \n",
    "                print(f\"‚úÖ Padded embedding from {len(base_embedding)} to {len(padded_embedding)} dimensions\")\n",
    "                return padded_embedding\n",
    "            \n",
    "            return base_embedding\n",
    "        else:\n",
    "            print(f\"‚ùå Embedding generation failed for text: {str(text_content)[:50]}...\")\n",
    "            return []\n",
    "\n",
    "    def _normalize_embedding(self, embedding: List[float]) -> List[float]:\n",
    "        \"\"\"Normalize embedding vector to unit length\"\"\"\n",
    "        import math\n",
    "        magnitude = math.sqrt(sum(x * x for x in embedding))\n",
    "        if magnitude > 0:\n",
    "            return [x / magnitude for x in embedding]\n",
    "        return embedding\n",
    "\n",
    "    def _test_connection(self) -> bool:\n",
    "        \"\"\"Test database connection and check for required tables\"\"\"\n",
    "        try:\n",
    "            conn = psycopg2.connect(self.database_url)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Check PostgreSQL version\n",
    "            cursor.execute(\"SELECT version();\")\n",
    "            version = cursor.fetchone()[0]\n",
    "            print(f\"‚úÖ Connected to: {version[:50]}...\")\n",
    "            \n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Database connection failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_table(self) -> bool:\n",
    "        \"\"\"Create required tables if they don't exist\"\"\"\n",
    "        try:\n",
    "            conn = psycopg2.connect(self.database_url)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Enable vector extension\n",
    "            cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "            \n",
    "            # Create temp_transactions table\n",
    "            cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS temp_transactions (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    content TEXT NOT NULL,\n",
    "                    embedding vector(1536),\n",
    "                    metadata JSONB,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "            # Create master_vector table\n",
    "            cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS master_vector (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    content TEXT NOT NULL,\n",
    "                    embedding vector(1536),\n",
    "                    metadata JSONB,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "            # Create search_history table\n",
    "            cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS search_history (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    content TEXT NOT NULL,\n",
    "                    embedding vector(1536),\n",
    "                    metadata JSONB,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "            # Create indexes for better performance\n",
    "            cursor.execute(\"\"\"\n",
    "                CREATE INDEX IF NOT EXISTS temp_transactions_embedding_idx \n",
    "                ON temp_transactions USING ivfflat (embedding vector_cosine_ops) \n",
    "                WITH (lists = 100);\n",
    "            \"\"\")\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                CREATE INDEX IF NOT EXISTS master_vector_embedding_idx \n",
    "                ON master_vector USING ivfflat (embedding vector_cosine_ops) \n",
    "                WITH (lists = 100);\n",
    "            \"\"\")\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                CREATE INDEX IF NOT EXISTS search_history_embedding_idx \n",
    "                ON search_history USING ivfflat (embedding vector_cosine_ops) \n",
    "                WITH (lists = 100);\n",
    "            \"\"\")\n",
    "            \n",
    "            conn.commit()\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            \n",
    "            print(\"‚úÖ All tables and indexes created successfully\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create tables: {e}\")\n",
    "            if 'conn' in locals():\n",
    "                conn.rollback()\n",
    "                cursor.close()\n",
    "                conn.close()\n",
    "            return False\n",
    "\n",
    "    def add_document(self, text: Union[str, dict], metadata: Optional[Dict[str, Any]] = None, table_name: str = \"temp_transactions\") -> bool:\n",
    "        \"\"\"Add document to specified table (convenience method)\"\"\"\n",
    "        if table_name == \"temp_transactions\":\n",
    "            return self.insert_temp_transaction(text, metadata)\n",
    "        elif table_name == \"master_vector\":\n",
    "            return self.insert_master_vector(text, metadata)\n",
    "        elif table_name == \"search_history\":\n",
    "            return self.insert_search_history(text, metadata)\n",
    "        else:\n",
    "            print(f\"‚ùå Invalid table name: {table_name}\")\n",
    "            return False\n",
    "    \n",
    "    def insert_temp_transaction(self, content: Union[str, dict], metadata: Optional[Dict[str, Any]] = None) -> bool:\n",
    "        \"\"\"Insert document into temp_transactions table - now handles both strings and dicts\"\"\"\n",
    "        # Convert content to string if it's a dict\n",
    "        if isinstance(content, dict):\n",
    "            # Create a meaningful string representation\n",
    "            content_str = f\"Transaction: {content.get('Description', 'N/A')} at {content.get('Vendor', 'N/A')} for ${content.get('Amount', 'N/A')}\"\n",
    "            # Add the original dict to metadata\n",
    "            if metadata is None:\n",
    "                metadata = {}\n",
    "            metadata.update(content)\n",
    "        else:\n",
    "            content_str = str(content)\n",
    "        \n",
    "        # Fixed syntax error: was self.(content_str), now self.get_embedding(content_str)\n",
    "        embedding = self.get_embedding(content_str)\n",
    "        if not embedding or len(embedding) != self.embedding_dimensions:\n",
    "            print(f\"‚ùå Invalid embedding: expected {self.embedding_dimensions} dimensions, got {len(embedding) if embedding else 0}\")\n",
    "            return False\n",
    "        \n",
    "        # Normalize the embedding before insertion\n",
    "        embedding = self._normalize_embedding(embedding)\n",
    "        \n",
    "        try:\n",
    "            conn = psycopg2.connect(self.database_url)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            insert_sql = \"\"\"\n",
    "            INSERT INTO temp_transactions (content, embedding, metadata)\n",
    "            VALUES (%s, %s::vector, %s)\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(insert_sql, (content_str, str(embedding), json.dumps(metadata or {})))\n",
    "            conn.commit()\n",
    "            \n",
    "            print(f\"‚úÖ Temp transaction inserted: '{content_str[:50]}...'\")\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to insert temp transaction: {e}\")\n",
    "            if 'conn' in locals():\n",
    "                conn.rollback()\n",
    "                cursor.close()\n",
    "                conn.close()\n",
    "            return False\n",
    "    def get_master_vector_count(self):\n",
    "        with self.engine.connect() as connection:\n",
    "            result = connection.execute(text(\"SELECT COUNT(*) FROM master_vector\"))\n",
    "            count = result.scalar()  # Returns single value\n",
    "            return count if count else 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def insert_master_vector(self, content: Union[str, dict], metadata: Optional[Dict[str, Any]] = None) -> bool:\n",
    "        \"\"\"Insert document into master_vector table, avoiding duplicates based on Category, Entity/Keyword, and Type\"\"\"\n",
    "        # Step 1: Normalize input\n",
    "        if isinstance(content, dict):\n",
    "            category = content.get('Category', 'N/A')\n",
    "            entity = content.get('Entity/Keyword', 'N/A')\n",
    "            txn_type = content.get('Type', 'N/A')\n",
    "            content_str = f\"Transaction: {category} at {entity} for ${txn_type}\"\n",
    "            metadata = {**(metadata or {}), **content}\n",
    "        else:\n",
    "            print(\"‚ùå Skipping insert: content is not a dictionary, cannot check for duplicates.\")\n",
    "            return False\n",
    "    \n",
    "        # Step 2: Check for duplicate\n",
    "        try:\n",
    "            with psycopg2.connect(self.database_url) as conn:\n",
    "                with conn.cursor() as cursor:\n",
    "                    duplicate_check_sql = \"\"\"\n",
    "                    SELECT 1 FROM master_vector\n",
    "                    WHERE \n",
    "                        metadata->>'Category' = %s AND \n",
    "                        metadata->>'Entity/Keyword' = %s AND \n",
    "                        metadata->>'Type' = %s\n",
    "                    LIMIT 1\n",
    "                    \"\"\"\n",
    "                    cursor.execute(duplicate_check_sql, (category, entity, txn_type))\n",
    "                    if cursor.fetchone():\n",
    "                        print(f\"‚ö†Ô∏è Duplicate found: {category}, {entity}, {txn_type} ‚Äî skipping insert.\")\n",
    "                        return False\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during duplicate check: {e}\")\n",
    "            return False\n",
    "    \n",
    "        # Step 3: Generate and validate embedding\n",
    "        embedding = self.get_embedding(content_str)\n",
    "        if not embedding or len(embedding) != self.embedding_dimensions:\n",
    "            print(f\"‚ùå Invalid embedding: expected {self.embedding_dimensions} dimensions, got {len(embedding) if embedding else 0}\")\n",
    "            return False\n",
    "    \n",
    "        embedding = self._normalize_embedding(embedding)\n",
    "        embedding_str = '[' + ','.join(map(str, embedding)) + ']'\n",
    "    \n",
    "        # Step 4: Insert\n",
    "        insert_sql = \"\"\"\n",
    "        INSERT INTO master_vector (content, embedding, metadata)\n",
    "        VALUES (%s, %s::vector, %s)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with psycopg2.connect(self.database_url) as conn:\n",
    "                with conn.cursor() as cursor:\n",
    "                    cursor.execute(insert_sql, (content_str, embedding_str, json.dumps(metadata or {})))\n",
    "                conn.commit()\n",
    "            print(f\"‚úÖ Master vector inserted: '{content_str[:50]}...'\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to insert master vector: {e}\")\n",
    "            return False\n",
    "\n",
    "    def insert_search_history(self, query: Union[str, dict], metadata: Optional[Dict[str, Any]] = None) -> bool:\n",
    "        \"\"\"Insert search query into search_history table\"\"\"\n",
    "        # Convert query to string if it's a dict\n",
    "        if isinstance(query, dict):\n",
    "            query_str = str(query)\n",
    "            if metadata is None:\n",
    "                metadata = {}\n",
    "            metadata.update(query)\n",
    "        else:\n",
    "            query_str = str(query)\n",
    "            \n",
    "        embedding = self.get_embedding(query_str)\n",
    "        if not embedding or len(embedding) != self.embedding_dimensions:\n",
    "            print(f\"‚ùå Invalid embedding: expected {self.embedding_dimensions} dimensions, got {len(embedding) if embedding else 0}\")\n",
    "            return False\n",
    "            \n",
    "        # Normalize the embedding before insertion\n",
    "        embedding = self._normalize_embedding(embedding)\n",
    "        \n",
    "        try:\n",
    "            conn = psycopg2.connect(self.database_url)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Add timestamp to metadata if not present\n",
    "            if metadata is None:\n",
    "                metadata = {}\n",
    "            if 'query_time' not in metadata:\n",
    "                metadata['query_time'] = datetime.now().isoformat()\n",
    "            \n",
    "            insert_sql = \"\"\"\n",
    "            INSERT INTO search_history (content, embedding, metadata)\n",
    "            VALUES (%s, %s::vector, %s)\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(insert_sql, (query_str, str(embedding), json.dumps(metadata)))\n",
    "            conn.commit()\n",
    "            \n",
    "            print(f\"‚úÖ Search history inserted: '{query_str[:50]}...'\")\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to insert search history: {e}\")\n",
    "            if 'conn' in locals():\n",
    "                conn.rollback()\n",
    "                cursor.close()\n",
    "                conn.close()\n",
    "            return False\n",
    "    \n",
    "    def search_similar(self, query: str, table_name: str = \"temp_transactions\", limit: int = 3) -> List[Dict]:\n",
    "        \"\"\"Search for similar documents in specified table\"\"\"\n",
    "        if table_name not in ['master_vector', 'search_history','temp_transactions']:\n",
    "            print(f\"‚ùå Invalid table name: {table_name}\")\n",
    "            return []\n",
    "            \n",
    "        query_embedding = self.get_embedding(query)\n",
    "        if not query_embedding:\n",
    "            return []\n",
    "        \n",
    "        # Normalize query embedding\n",
    "        query_embedding = self._normalize_embedding(query_embedding)\n",
    "        \n",
    "        try:\n",
    "            conn = psycopg2.connect(self.database_url)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            search_sql = f\"\"\"\n",
    "            SELECT content, metadata, \n",
    "                   1 - (embedding <=> %s::vector) as similarity\n",
    "            FROM {table_name}\n",
    "            ORDER BY embedding <=> %s::vector\n",
    "            LIMIT %s\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(search_sql, (str(query_embedding), str(query_embedding), limit))\n",
    "            results = cursor.fetchall()\n",
    "            \n",
    "            formatted_results = []\n",
    "            for row in results:\n",
    "                formatted_results.append({\n",
    "                    \"content\": row[0],\n",
    "                    \"metadata\": row[1],\n",
    "                    \"similarity\": float(row[2])\n",
    "                })\n",
    "            \n",
    "            print(f\"‚úÖ Found {len(formatted_results)} similar documents in {table_name}\")\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            return formatted_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Search failed: {e}\")\n",
    "            if 'conn' in locals():\n",
    "                cursor.close()\n",
    "                conn.close()\n",
    "            return []\n",
    "\n",
    "    def process_file(self, file_path: str, table_name: str = \"temp_transactions\") -> bool:\n",
    "        \"\"\"Process and insert data from a CSV file\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"‚ùå File not found: {file_path}\")\n",
    "                return False\n",
    "            \n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"‚úÖ Loaded {len(df)} records from {file_path}\")\n",
    "            \n",
    "            # Convert to dictionary records\n",
    "            records = df.to_dict(orient='records')\n",
    "            \n",
    "            success_count = 0\n",
    "            for i, record in enumerate(records):\n",
    "                print(f\"üîÑ Processing record {i+1}/{len(records)}...\")\n",
    "                \n",
    "                if self.add_document(record, {\"source\": file_path, \"row_id\": i}, table_name):\n",
    "                    success_count += 1\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            print(f\"‚úÖ Successfully processed {success_count}/{len(records)} records\")\n",
    "            return success_count == len(records)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to process file {file_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def find_best_classification_match(self, text: str, similarity_threshold: float = 0.7) -> Dict:\n",
    "        \"\"\"\n",
    "        Find the best classification match from master_vector and search_history tables.\n",
    "        Priority: master_vector first, then search_history if no match found.\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            \"classification_category\": None,\n",
    "            \"reason_source\": None,\n",
    "            \"confidence_score\": 0.0\n",
    "        }\n",
    "        \n",
    "        # First, search in master_vector table\n",
    "        master_matches = self.search_similar(text, table_name=\"master_vector\", limit=1)\n",
    "        \n",
    "        if master_matches and len(master_matches) > 0:\n",
    "            best_match = master_matches[0]\n",
    "            if best_match[\"similarity\"] >= similarity_threshold:\n",
    "                # Extract classification category from metadata\n",
    "                metadata = best_match.get(\"metadata\", {})\n",
    "                if isinstance(metadata, dict):\n",
    "                    classification = metadata.get(\"classification_category\") or metadata.get(\"category\") or metadata.get(\"Entity/Keyword\")\n",
    "                else:\n",
    "                    # If metadata is a string, try to parse it or use content\n",
    "                    classification = self._extract_category_from_content(best_match[\"content\"])\n",
    "                \n",
    "                result = {\n",
    "                    \"classification_category\": classification,\n",
    "                    \"reason_source\": \"master_vector\",\n",
    "                    \"confidence_score\": best_match[\"similarity\"]\n",
    "                }\n",
    "                return result\n",
    "        \n",
    "        # If no match in master_vector, search in search_history\n",
    "        history_matches = self.search_similar(text, table_name=\"search_history\", limit=1)\n",
    "        \n",
    "        if history_matches and len(history_matches) > 0:\n",
    "            best_match = history_matches[0]\n",
    "            if best_match[\"similarity\"] >= similarity_threshold:\n",
    "                # Extract classification category from metadata\n",
    "                metadata = best_match.get(\"metadata\", {})\n",
    "                if isinstance(metadata, dict):\n",
    "                    classification = metadata.get(\"classification_category\") or metadata.get(\"category\")\n",
    "                else:\n",
    "                    # If metadata is a string, try to parse it or use content\n",
    "                    classification = self._extract_category_from_content(best_match[\"content\"])\n",
    "                \n",
    "                result = {\n",
    "                    \"classification_category\": classification,\n",
    "                    \"reason_source\": \"search_history\",\n",
    "                    \"confidence_score\": best_match[\"similarity\"]\n",
    "                }\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _extract_category_from_content(self, content: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract category from content string if metadata doesn't contain it.\n",
    "        This is a fallback method - adjust based on your data structure.\n",
    "        \"\"\"\n",
    "        if not content:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Simple heuristic - you may need to adjust this based on your data format\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        # Common transaction categories\n",
    "        if any(word in content_lower for word in ['grocery', 'food', 'restaurant', 'dining']):\n",
    "            return \"Food & Dining\"\n",
    "        elif any(word in content_lower for word in ['gas', 'fuel', 'station']):\n",
    "            return \"Transportation\"\n",
    "        elif any(word in content_lower for word in ['pharmacy', 'medical', 'health']):\n",
    "            return \"Healthcare\"\n",
    "        elif any(word in content_lower for word in ['amazon', 'walmart', 'target', 'shopping']):\n",
    "            return \"Shopping\"\n",
    "        elif any(word in content_lower for word in ['bank', 'atm', 'transfer']):\n",
    "            return \"Banking\"\n",
    "        else:\n",
    "            return \"Other\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine 'Description' and 'Vendor' for embedding\n",
    "def get_text_to_embed(row):\n",
    "    desc = str(row['Description']).strip()\n",
    "    vendor = str(row['Vendor']).strip()\n",
    "    return f\"{desc} - {vendor}\" if vendor else desc\n",
    "\n",
    "# Main processing code\n",
    "def process_transactions_with_classification(input_file):\n",
    "    # Load CSV with only required columns\n",
    "    test_docs = pd.read_csv(input_file, usecols=['TransactionID', 'Description', 'Vendor'])\n",
    "    print(\"üìÅ Loaded transaction data:\")\n",
    "    print(test_docs.head())\n",
    "    \n",
    "    # Initialize vector store\n",
    "    database_url = setup_database_url()\n",
    "    vector_store = DatabaseVectorStore(database_url)\n",
    "    \n",
    "    # Check if master_vector has data\n",
    "    master_count = vector_store.get_master_vector_count()\n",
    "    print(f\"üìä Master vector table contains {master_count} entries\")\n",
    "    \n",
    "    # Initialize lists for new data\n",
    "    embeddings = []\n",
    "    classifications = []\n",
    "    reason_sources = []\n",
    "    confidence_scores = []\n",
    "    \n",
    "    # Process each transaction\n",
    "    print(\"üîÑ Processing transactions...\")\n",
    "    for _, row in tqdm(test_docs.iterrows(), total=len(test_docs), desc=\"Processing transactions\"):\n",
    "        # Generate text for embedding\n",
    "        text_input = get_text_to_embed(row)\n",
    "        \n",
    "        # Generate embedding\n",
    "        emb = vector_store.get_embedding(text_input)\n",
    "        embeddings.append(emb)\n",
    "        \n",
    "        # Find classification match\n",
    "        classification_result = vector_store.find_best_classification_match(text_input)\n",
    "        print(classification_result)\n",
    "        \n",
    "        classifications.append(classification_result[\"classification_category\"])\n",
    "        reason_sources.append(classification_result[\"reason_source\"])\n",
    "        confidence_scores.append(classification_result[\"confidence_score\"])\n",
    "    \n",
    "    # print(classifications)\n",
    "    # Add all new columns to dataframe\n",
    "    # test_docs[\"embedding\"] = embeddings\n",
    "    test_docs[\"classification_category\"] = classifications\n",
    "    test_docs[\"reason_source\"] = reason_sources\n",
    "    test_docs[\"confidence_score\"] = confidence_scores\n",
    "    \n",
    "    # Add embedding length for verification\n",
    "    #test_docs[\"embedding_len\"] = test_docs[\"embedding\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüìã Processing Results:\")\n",
    "    display_columns = [\"TransactionID\", \"Description\", \"Vendor\", \"classification_category\", \n",
    "                      \"reason_source\", \"confidence_score\"]\n",
    "    print(test_docs[display_columns].head(10))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìä Classification Summary:\")\n",
    "    classification_summary = test_docs.groupby(['classification_category', 'reason_source']).size().reset_index(name='count')\n",
    "    print(classification_summary)\n",
    "    \n",
    "    # Save results\n",
    "    output_file = 'bank_transactions_classified.csv'\n",
    "    test_docs.to_csv(output_file, index=False)\n",
    "    print(f\"üíæ Results saved to {output_file}\")\n",
    "    \n",
    "    return test_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main function to demonstrate functionalit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to demonstrate functionality\"\"\"\n",
    "    print(\"üöÄ Starting GenAI Vector Store Demo...\")\n",
    "    \n",
    "    \n",
    "    # Process bank transactions (if file exists)\n",
    "    # transaction_results = process_bank_transactions()\n",
    "    transaction_results =process_bank_transactions(limit=50, batch_size=5)\n",
    "    \n",
    "    # # Test simple vector store (always works)\n",
    "    # print(\"\\n\" + \"=\"*50)\n",
    "    # print(\"TESTING SIMPLE VECTOR STORE (In-Memory)\")\n",
    "    # print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        simple_store = SimpleVectorStore()\n",
    "        #simple_store.test_functionality()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå SimpleVectorStore failed: {e}\")\n",
    "    \n",
    "    # Setup database URL interactively if not set\n",
    "    database_url = setup_database_url()\n",
    "\n",
    "      \n",
    "    if database_url and database_url.startswith(\"postgresql\"):\n",
    "        # Test PostgreSQL vector store\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TESTING POSTGRESQL VECTOR STORE\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            db_store = DatabaseVectorStore(database_url)\n",
    "            \n",
    "            # Create table\n",
    "            if db_store.create_table():\n",
    "            \n",
    "                test_docs_master=pd.read_csv('Master_data.csv')\n",
    "                csv_count = len(test_docs_master)\n",
    "\n",
    "                test_docs_master=test_docs_master.to_dict(orient='records')\n",
    "                table_count = db_store.get_master_vector_count()  # Assume this returns an int\n",
    "                # print(table_count,'------------------->imran<--------------')\n",
    "                if csv_count > table_count:\n",
    "                    print(\"New records detected. Inserting into DB...\")\n",
    "                    for doc_master in test_docs_master:\n",
    "                        db_store.insert_master_vector(doc_master, {\"source\": \"demo\"})\n",
    "                else:\n",
    "                    print(\"No new records to insert. Running in test mode.\")\n",
    "\n",
    "                test_docs = pd.read_csv('bank_transactions_with_vendor_100.csv', usecols=['TransactionID', 'Description', 'Vendor'])\n",
    "                print(test_docs.head())\n",
    "                input_file='bank_transactions_with_vendor_100.csv'\n",
    "                processed_data = process_transactions_with_classification(input_file)\n",
    "                \n",
    "                # Search\n",
    "                results = db_store.search_similar(\"I want to retrieve all transaction details related to Zomato, including a clear summary of the total inflow and outflow amounts, categorized by credit and debit. Additionally, the summary should include transaction models such as UPT, NTF, etc.\", limit=2)\n",
    "                # if results:\n",
    "                #     print(\"‚úÖ Database vector store test successful!\")\n",
    "                #     for result in results:\n",
    "                #         print(f\"   Similarity: {result['similarity']:.3f} - {result['content'][:50]}...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå DatabaseVectorStore failed: {e}\")\n",
    "            print(\"üí° Try using SQLite option or check PostgreSQL setup\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Database tests skipped\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Demo completed!\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function Call**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting GenAI Vector Store Demo...\n",
      "‚úÖ Loaded 20 transactions\n",
      "üìä Processing first 20 transactions\n",
      "\n",
      "üì¶ Processing batch 1: transactions 1-5\n",
      "üîÑ Processing transaction 1: UPI Payment to Zomato...\n",
      "‚úÖ Category: Entertainment\n",
      "üîÑ Processing transaction 2: EMI payment - ICICI Personal Loan...\n",
      "‚úÖ Category: Other\n",
      "üîÑ Processing transaction 3: UPI Payment to Zomato...\n",
      "‚úÖ Category: Entertainment\n",
      "üîÑ Processing transaction 4: EMI payment - ICICI Personal Loan...\n",
      "‚úÖ Category: Other\n",
      "üîÑ Processing transaction 5: EMI payment - ICICI Personal Loan...\n",
      "‚úÖ Category: Other\n",
      "\n",
      "üì¶ Processing batch 2: transactions 6-10\n",
      "üîÑ Processing transaction 6: UPI Payment to Zomato...\n",
      "‚úÖ Category: Entertainment\n",
      "üîÑ Processing transaction 7: Interest credited on SB account...\n",
      "‚úÖ Category: Other\n",
      "üîÑ Processing transaction 8: Received from Mahesh via UPI...\n",
      "‚úÖ Category: Other\n",
      "üîÑ Processing transaction 9: Interest credited on SB account...\n",
      "‚úÖ Category: Other\n",
      "üîÑ Processing transaction 10: EMI payment - ICICI Personal Loan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Proactive rate limit wait: 54.0 seconds...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Category: Other\n",
      "\n",
      "üì¶ Processing batch 3: transactions 11-15\n",
      "üîÑ Processing transaction 11: UPI Payment to Zomato...\n",
      "‚úÖ Category: Entertainment\n",
      "üîÑ Processing transaction 12: UPI Payment to Zomato...\n",
      "‚úÖ Category: Entertainment\n",
      "üîÑ Processing transaction 13: Received from Mahesh via UPI...\n",
      "‚úÖ Category: Other\n",
      "üîÑ Processing transaction 14: Received from Mahesh via UPI...\n",
      "‚úÖ Category: Other\n",
      "üîÑ Processing transaction 15: EMI payment - ICICI Personal Loan...\n",
      "‚úÖ Category: Other\n",
      "\n",
      "üì¶ Processing batch 4: transactions 16-20\n",
      "üîÑ Processing transaction 16: UPI Payment to Zomato...\n",
      "‚úÖ Category: Entertainment\n",
      "üîÑ Processing transaction 17: EMI payment - ICICI Personal Loan...\n",
      "‚úÖ Category: Other\n",
      "üîÑ Processing transaction 18: Interest credited on SB account...\n",
      "‚úÖ Category: Other\n",
      "üîÑ Processing transaction 19: EMI payment - ICICI Personal Loan...\n",
      "‚úÖ Category: Other\n",
      "üîÑ Processing transaction 20: EMI payment - ICICI Personal Loan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Category: Other\n",
      "\n",
      "‚úÖ Processing complete! Processed 20 transactions\n",
      "[{'transaction_id': 1, 'description': 'UPI Payment to Zomato', 'vendor': 'Zomato', 'amount': '431', 'category': 'Entertainment'}, {'transaction_id': 2, 'description': 'EMI payment - ICICI Personal Loan', 'vendor': 'ICICI Personal Loan', 'amount': '1,770', 'category': 'Other'}, {'transaction_id': 3, 'description': 'UPI Payment to Zomato', 'vendor': 'Zomato', 'amount': '134', 'category': 'Entertainment'}, {'transaction_id': 4, 'description': 'EMI payment - ICICI Personal Loan', 'vendor': 'ICICI Personal Loan', 'amount': '4,725', 'category': 'Other'}, {'transaction_id': 5, 'description': 'EMI payment - ICICI Personal Loan', 'vendor': 'ICICI Personal Loan', 'amount': '1,203', 'category': 'Other'}, {'transaction_id': 6, 'description': 'UPI Payment to Zomato', 'vendor': 'Zomato', 'amount': '147', 'category': 'Entertainment'}, {'transaction_id': 7, 'description': 'Interest credited on SB account', 'vendor': 'Interest', 'amount': '32', 'category': 'Other'}, {'transaction_id': 8, 'description': 'Received from Mahesh via UPI', 'vendor': 'Received', 'amount': '838.62', 'category': 'Other'}, {'transaction_id': 9, 'description': 'Interest credited on SB account', 'vendor': 'Interest', 'amount': '50', 'category': 'Other'}, {'transaction_id': 10, 'description': 'EMI payment - ICICI Personal Loan', 'vendor': 'ICICI Personal Loan', 'amount': '1,100', 'category': 'Other'}, {'transaction_id': 11, 'description': 'UPI Payment to Zomato', 'vendor': 'Zomato', 'amount': '299', 'category': 'Entertainment'}, {'transaction_id': 12, 'description': 'UPI Payment to Zomato', 'vendor': 'Zomato', 'amount': '249', 'category': 'Entertainment'}, {'transaction_id': 13, 'description': 'Received from Mahesh via UPI', 'vendor': 'Received', 'amount': '1,000', 'category': 'Other'}, {'transaction_id': 14, 'description': 'Received from Mahesh via UPI', 'vendor': 'Received', 'amount': '711', 'category': 'Other'}, {'transaction_id': 15, 'description': 'EMI payment - ICICI Personal Loan', 'vendor': 'ICICI Personal Loan', 'amount': '1,395.04', 'category': 'Other'}, {'transaction_id': 16, 'description': 'UPI Payment to Zomato', 'vendor': 'Zomato', 'amount': '147', 'category': 'Entertainment'}, {'transaction_id': 17, 'description': 'EMI payment - ICICI Personal Loan', 'vendor': 'ICICI Personal Loan', 'amount': '3,195', 'category': 'Other'}, {'transaction_id': 18, 'description': 'Interest credited on SB account', 'vendor': 'Interest', 'amount': '30', 'category': 'Other'}, {'transaction_id': 19, 'description': 'EMI payment - ICICI Personal Loan', 'vendor': 'ICICI Personal Loan', 'amount': '2,299', 'category': 'Other'}, {'transaction_id': 20, 'description': 'EMI payment - ICICI Personal Loan', 'vendor': 'ICICI Personal Loan', 'amount': '1,200', 'category': 'Other'}]\n",
      "üîß Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SimpleVectorStore initialized with SentenceTransformer\n",
      "\n",
      "==================================================\n",
      "TESTING POSTGRESQL VECTOR STORE\n",
      "==================================================\n",
      "üîß Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to: PostgreSQL 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1) on ...\n",
      "‚úÖ DatabaseVectorStore initialized with SentenceTransformer\n",
      "‚úÖ All tables and indexes created successfully\n",
      "No new records to insert. Running in test mode.\n",
      "  TransactionID                        Description               Vendor\n",
      "0       T781784              UPI Payment to Zomato               Zomato\n",
      "1       T781785  EMI payment - ICICI Personal Loan  ICICI Personal Loan\n",
      "2       T781786              UPI Payment to Zomato               Zomato\n",
      "3       T781787  EMI payment - ICICI Personal Loan  ICICI Personal Loan\n",
      "4       T781788  EMI payment - ICICI Personal Loan  ICICI Personal Loan\n",
      "üìÅ Loaded transaction data:\n",
      "  TransactionID                        Description               Vendor\n",
      "0       T781784              UPI Payment to Zomato               Zomato\n",
      "1       T781785  EMI payment - ICICI Personal Loan  ICICI Personal Loan\n",
      "2       T781786              UPI Payment to Zomato               Zomato\n",
      "3       T781787  EMI payment - ICICI Personal Loan  ICICI Personal Loan\n",
      "4       T781788  EMI payment - ICICI Personal Loan  ICICI Personal Loan\n",
      "üîß Using device: cuda\n",
      "‚úÖ Connected to: PostgreSQL 16.9 (Ubuntu 16.9-0ubuntu0.24.04.1) on ...\n",
      "‚úÖ DatabaseVectorStore initialized with SentenceTransformer\n",
      "üìä Master vector table contains 26 entries\n",
      "üîÑ Processing transactions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:   0%|                                                                                               | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e01136e9f9419c94edb73cbe50a7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9488e56e1f4265ba746054e785c15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 1 similar documents in master_vector\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e21dc6fffb141f99d882a1252251baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:   5%|‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                  | 1/20 [00:00<00:03,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 0 similar documents in search_history\n",
      "{'classification_category': None, 'reason_source': None, 'confidence_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8b8dce56004064be7f5a0771cedcc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213cf7a26dcb4a4a95d117570a1fbf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                              | 2/20 [00:00<00:02,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 1 similar documents in master_vector\n",
      "{'classification_category': 'ICICI Loan EMI', 'reason_source': 'master_vector', 'confidence_score': 0.7745476499469502}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117f282c9d83494f86a49b0108b1309f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eecbb09610b43a886af6e4c613affc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 1 similar documents in master_vector\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec6267362e5450c97fab87fd73c99ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                          | 3/20 [00:00<00:02,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 0 similar documents in search_history\n",
      "{'classification_category': None, 'reason_source': None, 'confidence_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c381bc3cb34075a6dcead789fde877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a25c591d9c4adcbf84592c0d55ecdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 1 similar documents in master_vector\n",
      "{'classification_category': 'ICICI Loan EMI', 'reason_source': 'master_vector', 'confidence_score': 0.7745476499469502}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49acb02594c5451c9296ff05dd9f2e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779bc3b98bc64f16ad1f661d1d8e5c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                 | 5/20 [00:00<00:01, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 1 similar documents in master_vector\n",
      "{'classification_category': 'ICICI Loan EMI', 'reason_source': 'master_vector', 'confidence_score': 0.7745476499469502}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4940c90b11c4a35aebe03e9d8f8f8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf1a151af75494cb7088304c9213646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 1 similar documents in master_vector\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079c43a2193f46b7a203ab22c8cc7006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 0 similar documents in search_history\n",
      "{'classification_category': None, 'reason_source': None, 'confidence_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6eaf1c85b1a41cebf2b410e7eda1188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a751c48122674d0aa6922267984a5eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 1 similar documents in master_vector\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090a20194bd2419c9ff8121f2c6f8cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                        | 7/20 [00:00<00:01, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 0 similar documents in search_history\n",
      "{'classification_category': None, 'reason_source': None, 'confidence_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab38da52091f4778837726eba1e6e5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f86922d29544b79986045cf55de1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 0 similar documents in master_vector\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ab82c65462432a9aaf9b0149a84306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 0 similar documents in search_history\n",
      "{'classification_category': None, 'reason_source': None, 'confidence_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49acfb04cd248bcb8e36a193312f734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ba837506fc434e8fd40e1005311cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 1 similar documents in master_vector\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfc9bdc9c454d9abcd3ccce486178c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                               | 9/20 [00:00<00:01,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 0 similar documents in search_history\n",
      "{'classification_category': None, 'reason_source': None, 'confidence_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff902bc0f5d74f45a037503cfe87d351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c1fa9789484a82b959d065460835f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 1 similar documents in master_vector\n",
      "{'classification_category': 'ICICI Loan EMI', 'reason_source': 'master_vector', 'confidence_score': 0.7745476499469502}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac925ffbeea84bb3b82c0c51f34d1b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54077fffe7d4cc78910f82a61733982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 1 similar documents in master_vector\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065944b74fd44735bfcb0db3dc920c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                      | 11/20 [00:01<00:01,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 0 similar documents in search_history\n",
      "{'classification_category': None, 'reason_source': None, 'confidence_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4ad9bf09df47a1aad08d805906aa57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebebf4f4d7e0491fa2fa05030cb99de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 1 similar documents in master_vector\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430b80db113448169d2a4ca6bf262e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                  | 12/20 [00:01<00:00,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 0 similar documents in search_history\n",
      "{'classification_category': None, 'reason_source': None, 'confidence_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea8295bc3b6482b9f1dc8aaed5d81bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e5a540bd964a11958fb7d1cf610bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 0 similar documents in master_vector\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98588afebb7e4c978498d046dfcbf6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 0 similar documents in search_history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                              | 13/20 [00:01<00:00,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classification_category': None, 'reason_source': None, 'confidence_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ada6113f57f49b9a5e3357b369b2479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf4e9be1c7a4bad9616d5149af68b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 0 similar documents in master_vector\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab6b620b06647dda37418f3b30ee107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 14/20 [00:01<00:00,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 0 similar documents in search_history\n",
      "{'classification_category': None, 'reason_source': None, 'confidence_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89f68e1491c47f28aec48edbb6c51e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed9d8ef14084e19948658630919e4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 15/20 [00:01<00:00,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 1 similar documents in master_vector\n",
      "{'classification_category': 'ICICI Loan EMI', 'reason_source': 'master_vector', 'confidence_score': 0.7745476499469502}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29287d13760434286c25f3179d55e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828e0709fe34453c8dde11fda5b491f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 1 similar documents in master_vector\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2cc248759b450c8417ac00710cb6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                 | 16/20 [00:01<00:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 0 similar documents in search_history\n",
      "{'classification_category': None, 'reason_source': None, 'confidence_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fcaac6f4a4406da59b1b53ab017aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec29bcf6b7da4a3cbfb3202e8cf07ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 17/20 [00:02<00:00,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 1 similar documents in master_vector\n",
      "{'classification_category': 'ICICI Loan EMI', 'reason_source': 'master_vector', 'confidence_score': 0.7745476499469502}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06845a960a7442f8b8fc4e4d6a3fc44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fd022cf1fd4ce58779aa7e70f60a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 1 similar documents in master_vector\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c21ee009c4d47638528e091b8e6919c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 18/20 [00:02<00:00,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 0 similar documents in search_history\n",
      "{'classification_category': None, 'reason_source': None, 'confidence_score': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de26d05b15b246028f6b9ecfb6fcee2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64ed76e5bc0482092091df72ab1a3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 19/20 [00:02<00:00,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 1 similar documents in master_vector\n",
      "{'classification_category': 'ICICI Loan EMI', 'reason_source': 'master_vector', 'confidence_score': 0.7745476499469502}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d6358416584399bff24e169f2386b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b5dfe99e044dc59ec7a6c786a2339a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing transactions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 1 similar documents in master_vector\n",
      "{'classification_category': 'ICICI Loan EMI', 'reason_source': 'master_vector', 'confidence_score': 0.7745476499469502}\n",
      "\n",
      "üìã Processing Results:\n",
      "  TransactionID                        Description               Vendor  \\\n",
      "0       T781784              UPI Payment to Zomato               Zomato   \n",
      "1       T781785  EMI payment - ICICI Personal Loan  ICICI Personal Loan   \n",
      "2       T781786              UPI Payment to Zomato               Zomato   \n",
      "3       T781787  EMI payment - ICICI Personal Loan  ICICI Personal Loan   \n",
      "4       T781788  EMI payment - ICICI Personal Loan  ICICI Personal Loan   \n",
      "5       T781789              UPI Payment to Zomato               Zomato   \n",
      "6       T781790    Interest credited on SB account             Interest   \n",
      "7       T781791       Received from Mahesh via UPI             Received   \n",
      "8       T781792    Interest credited on SB account             Interest   \n",
      "9       T781793  EMI payment - ICICI Personal Loan  ICICI Personal Loan   \n",
      "\n",
      "  classification_category  reason_source  confidence_score  \n",
      "0                    None           None          0.000000  \n",
      "1          ICICI Loan EMI  master_vector          0.774548  \n",
      "2                    None           None          0.000000  \n",
      "3          ICICI Loan EMI  master_vector          0.774548  \n",
      "4          ICICI Loan EMI  master_vector          0.774548  \n",
      "5                    None           None          0.000000  \n",
      "6                    None           None          0.000000  \n",
      "7                    None           None          0.000000  \n",
      "8                    None           None          0.000000  \n",
      "9          ICICI Loan EMI  master_vector          0.774548  \n",
      "\n",
      "üìä Classification Summary:\n",
      "  classification_category  reason_source  count\n",
      "0          ICICI Loan EMI  master_vector      8\n",
      "üíæ Results saved to bank_transactions_classified.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe4e2e544594d308c96b55802540c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Padded embedding from 384 to 1536 dimensions\n",
      "‚úÖ Found 0 similar documents in temp_transactions\n",
      "\n",
      "‚úÖ Demo completed!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Google Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "class GoogleCustomSearch:\n",
    "    def __init__(self, api_key: str, search_engine_id: str):\n",
    "        self.api_key = api_key\n",
    "        self.search_engine_id = search_engine_id\n",
    "        self.base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    \n",
    "    def search(self, query: str, num_results: int = 10, start_index: int = 1, \n",
    "           site_search: Optional[str] = None, exact_terms: Optional[str] = None,\n",
    "           exclude_terms: Optional[str] = None, file_type: Optional[str] = None,\n",
    "           date_restrict: Optional[str] = None) -> Dict:\n",
    "    \n",
    "        params = {\n",
    "            'key': self.api_key,\n",
    "            'cx': self.search_engine_id,\n",
    "            'q': query,\n",
    "            'num': min(num_results, 10),\n",
    "            'start': start_index\n",
    "        }\n",
    "    \n",
    "        if site_search:\n",
    "            params['siteSearch'] = site_search\n",
    "        if exact_terms:\n",
    "            params['exactTerms'] = exact_terms\n",
    "        if exclude_terms:\n",
    "            params['excludeTerms'] = exclude_terms\n",
    "        if file_type:\n",
    "            params['fileType'] = file_type\n",
    "        if date_restrict:\n",
    "            params['dateRestrict'] = date_restrict\n",
    "    \n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(self.base_url, params=params)\n",
    "                response.raise_for_status()\n",
    "                return response.json()\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if response.status_code == 429:\n",
    "                    wait = 2 ** attempt\n",
    "                    print(f\"Rate limit hit. Retrying in {wait} seconds...\")\n",
    "                    time.sleep(wait)\n",
    "                else:\n",
    "                    print(f\"HTTP error occurred: {e}\")\n",
    "                    break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error making request: {e}\")\n",
    "                time.sleep(3)\n",
    "                break\n",
    "        return {}\n",
    "    \n",
    "        \n",
    "    def get_search_results(self, query: str, max_results: int = 10) -> List[Dict]:\n",
    "        results = []\n",
    "        start_index = 1\n",
    "        attempt = 1\n",
    "        \n",
    "        while len(results) < max_results:\n",
    "            try:\n",
    "                batch_size = min(10, max_results - len(results))\n",
    "                response = self.search(query, num_results=batch_size, start_index=start_index)\n",
    "                \n",
    "                if 'items' not in response:\n",
    "                    break\n",
    "                \n",
    "                for item in response['items']:\n",
    "                    if len(results) >= max_results:\n",
    "                        break\n",
    "                    \n",
    "                    result = {\n",
    "                        'title': item.get('title', ''),\n",
    "                        'link': item.get('link', ''),\n",
    "                        'snippet': item.get('snippet', ''),\n",
    "                        'display_link': item.get('displayLink', ''),\n",
    "                        'formatted_url': item.get('formattedUrl', '')\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                \n",
    "                start_index += batch_size\n",
    "                attempt = 1  # Reset attempt after success\n",
    "            \n",
    "            except HTTPError as http_err:\n",
    "                if response.status_code == 429:\n",
    "                    print(f\"Rate limit hit (attempt {attempt})... Retrying after backoff.\")\n",
    "                    time.sleep(2 ** attempt)\n",
    "                    attempt += 1\n",
    "                else:\n",
    "                    print(f\"HTTP error occurred: {http_err}\")\n",
    "                    break\n",
    "            except Exception as err:\n",
    "                print(f\"Unexpected error: {err}\")\n",
    "                break\n",
    "            \n",
    "            if 'queries' not in response or 'nextPage' not in response['queries']:\n",
    "                break\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_results(self, query: str, max_results: int = 10):\n",
    "        results = self.get_search_results(query, max_results)\n",
    "        \n",
    "        print(f\"\\nSearch Results for: '{query}'\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. {result['title']}\")\n",
    "            print(f\"   URL: {result['link']}\")\n",
    "            print(f\"   {result['snippet']}\")\n",
    "            print(\"-\" * 40)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No results found.\")\n",
    "\n",
    "    def has_transaction_mention(self, query: str) -> bool:\n",
    "        results = self.get_search_results(query, max_results=1)\n",
    "        return bool(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Google Search Function Calling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Transaction 1 ---\n",
      "Searching for: AMAZON.COM PURCHASE\n",
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyDdeeMnCwvNiSDkqb2dvA0tvRG3ZbxQe4k&cx=a0fcf4aeb345944e6&q=AMAZON.COM+PURCHASE+company&num=3&start=1\n",
      "Detected Transaction Presence via Search: FALSE\n",
      "\n",
      "--- Processing Transaction 2 ---\n",
      "Searching for: Starbucks\n",
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyDdeeMnCwvNiSDkqb2dvA0tvRG3ZbxQe4k&cx=a0fcf4aeb345944e6&q=Starbucks+company&num=3&start=1\n",
      "Detected Transaction Presence via Search: FALSE\n",
      "\n",
      "--- Processing Transaction 3 ---\n",
      "Searching for: UBER TECHNOLOGIES\n",
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyDdeeMnCwvNiSDkqb2dvA0tvRG3ZbxQe4k&cx=a0fcf4aeb345944e6&q=UBER+TECHNOLOGIES+company&num=3&start=1\n",
      "Detected Transaction Presence via Search: FALSE\n",
      "\n",
      "--- Processing Transaction 4 ---\n",
      "Searching for: Flipkart UPI\n",
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyDdeeMnCwvNiSDkqb2dvA0tvRG3ZbxQe4k&cx=a0fcf4aeb345944e6&q=Flipkart+UPI+company&num=3&start=1\n",
      "Detected Transaction Presence via Search: FALSE\n",
      "\n",
      "--- Processing Transaction 5 ---\n",
      "Searching for: EMI payment - ICICI Personal Loan\n",
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyDdeeMnCwvNiSDkqb2dvA0tvRG3ZbxQe4k&cx=a0fcf4aeb345944e6&q=EMI+payment+-+ICICI+Personal+Loan+company&num=3&start=1\n",
      "Detected Transaction Presence via Search: FALSE\n",
      "\n",
      "--- Processing Transaction 6 ---\n",
      "Searching for: UPI Payment to Zomato\n",
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyDdeeMnCwvNiSDkqb2dvA0tvRG3ZbxQe4k&cx=a0fcf4aeb345944e6&q=UPI+Payment+to+Zomato+company&num=3&start=1\n",
      "Detected Transaction Presence via Search: FALSE\n",
      "\n",
      "--- Processing Transaction 7 ---\n",
      "Searching for: MONTHLY SUBSCRIPTION\n",
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://www.googleapis.com/customsearch/v1?key=AIzaSyDdeeMnCwvNiSDkqb2dvA0tvRG3ZbxQe4k&cx=a0fcf4aeb345944e6&q=MONTHLY+SUBSCRIPTION+company&num=3&start=1\n",
      "Detected Transaction Presence via Search: FALSE\n",
      "\n",
      "All enriched transactions:\n",
      "{'transaction_id': 1, 'description': 'AMAZON.COM PURCHASE', 'vendor': 'Unknown Vendor', 'amount': 45.99, 'category': 'Miscellaneous', 'confidence_score': 0.5}\n",
      "{'transaction_id': 2, 'description': 'STARBUCKS COFFEE', 'vendor': 'Starbucks', 'amount': 12.5, 'category': 'Miscellaneous', 'confidence_score': 0.5}\n",
      "{'transaction_id': 3, 'description': 'Unknown Transaction', 'vendor': 'UBER TECHNOLOGIES', 'amount': 18.75, 'category': 'Miscellaneous', 'confidence_score': 0.5}\n",
      "{'transaction_id': 4, 'description': 'Flipkart UPI', 'vendor': 'Unknown Vendor', 'amount': 250.0, 'category': 'Miscellaneous', 'confidence_score': 0.5}\n",
      "{'transaction_id': 5, 'description': 'EMI payment - ICICI Personal Loan', 'vendor': 'Unknown Vendor', 'amount': 2000.0, 'category': 'Miscellaneous', 'confidence_score': 0.5}\n",
      "{'transaction_id': 6, 'description': 'UPI Payment to Zomato', 'vendor': 'Unknown Vendor', 'amount': 340.0, 'category': 'Miscellaneous', 'confidence_score': 0.5}\n",
      "{'transaction_id': 7, 'description': 'MONTHLY SUBSCRIPTION', 'vendor': 'Unknown Vendor', 'amount': 9.99, 'category': 'Miscellaneous', 'confidence_score': 0.5}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import google.generativeai as genai\n",
    "    import json\n",
    "\n",
    "    # Replace with your actual API keys and import your GoogleCustomSearch client\n",
    "    # from your_module import GoogleCustomSearch, API_KEY, SEARCH_ENGINE_ID\n",
    "\n",
    "    genai.configure(api_key=API_KEY)\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n",
    "    sample_transactions = [\n",
    "        {\"Description\": \"AMAZON.COM PURCHASE\", \"Vendor\": \"\", \"Amount\": 45.99},\n",
    "        {\"Description\": \"STARBUCKS COFFEE\", \"Vendor\": \"Starbucks\", \"Amount\": 12.50},\n",
    "        {\"Description\": \"\", \"Vendor\": \"UBER TECHNOLOGIES\", \"Amount\": 18.75},\n",
    "        {\"Description\": \"Flipkart UPI\", \"Vendor\": \"\", \"Amount\": 250.00},\n",
    "        {\"Description\": \"EMI payment - ICICI Personal Loan\", \"Vendor\": \"\", \"Amount\": 2000.00},\n",
    "        {\"Description\": \"UPI Payment to Zomato\", \"Vendor\": \"\", \"Amount\": 340.00},\n",
    "        {\"Description\": \"MONTHLY SUBSCRIPTION\", \"Vendor\": \"\", \"Amount\": 9.99}\n",
    "    ]\n",
    "\n",
    "    search_client = GoogleCustomSearch(API_KEY, SEARCH_ENGINE_ID)\n",
    "    enriched_transactions = []\n",
    "\n",
    "    general_categorization_prompt = \"\"\"\n",
    "    You are a financial transaction categorization expert.\n",
    "    \n",
    "    Your task is to categorize transactions based on their description and vendor information.\n",
    "    Consider the context carefully and assign each transaction into an appropriate category such as Education, Health, Groceries, Utilities, Transportation, Entertainment, Financial, etc.\n",
    "    \n",
    "    For example:\n",
    "    - Transactions related to school, college, university, or school fees fall under \"Education\".\n",
    "    - Transactions related to operations, surgery, injury fall under \"Health\".\n",
    "    - Transactions related to Amazon, eBay, Walmart, Alibaba, Shopify, Etsy, Target, Flipkart, Rakuten, and Best Buy. under \"E-Commerce or Shopoinng\".\n",
    "    - Similarly, classify other transactions appropriately based on description and vendor.\n",
    "    \n",
    "    For each transaction, provide a JSON response with the following structure:\n",
    "    {\n",
    "        \"category\": \"string\",\n",
    "        \"type\": \"string\",          # One of \"Inflow\", \"Outflow\", or \"Inflow-Outflow\"\n",
    "        \"confidence\": float,       # A confidence score between 0 and 1\n",
    "        \"explanation\": \"string\"    # A brief explanation for the classification\n",
    "    }\n",
    "    \n",
    "    Make sure your response is a valid JSON object matching the above format exactly.\n",
    "    \n",
    "    Example response:\n",
    "    {\n",
    "        \"category\": \"Groceries\",\n",
    "        \"type\": \"Outflow\",\n",
    "        \"confidence\": 0.95,\n",
    "        \"explanation\": \"Transaction matches grocery store pattern based on description and vendor.\"\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    for idx, row in enumerate(sample_transactions):\n",
    "        description = row.get(\"Description\", \"\").strip()\n",
    "        vendor = row.get(\"Vendor\", \"\").strip()\n",
    "        amount = row.get(\"Amount\", 0)\n",
    "        search_term = vendor or description or \"Unknown\"\n",
    "\n",
    "        print(f\"\\n--- Processing Transaction {idx + 1} ---\")\n",
    "        print(f\"Searching for: {search_term}\")\n",
    "\n",
    "        # Search vendor or description context\n",
    "        search_results = search_client.get_search_results(f\"{search_term} company\", max_results=3)\n",
    "        snippet_text = \" \".join([res.get(\"snippet\", \"\") for res in search_results]).lower()\n",
    "\n",
    "        # Heuristic: determine if it's a known transaction or vendor\n",
    "        has_transaction = any(keyword in snippet_text for keyword in [\n",
    "            \"transaction\", \"payment\", \"transfer\", \"credited\", \"debited\", \"charge\", \"refund\",\n",
    "            \"reversal\", \"settlement\", \"invoice\", \"billed\", \"fee\", \"fine\", \"penalty\", \"adjustment\",\n",
    "            \"upi\", \"imps\", \"neft\", \"rtgs\", \"ach\", \"qr\", \"nfc\", \"gateway\", \"netbanking\",\n",
    "            \"wallet\", \"upi-collect\", \"upi-id\", \"scan\", \"pos\", \"atm\", \"card\", \"credit card\", \"debit card\",\n",
    "            \"loan\", \"emi\", \"installment\", \"interest\", \"mutual fund\", \"sip\", \"nps\", \"insurance\",\n",
    "            \"lic\", \"equity\", \"dividend\", \"stock\", \"investment\", \"pension\", \"brokerage\", \"buy\", \"sell\",\n",
    "            \"salary\", \"wages\", \"bonus\", \"incentive\", \"payout\", \"stipend\", \"reimbursement\", \"commission\",\n",
    "            \"purchase\", \"order\", \"subscription\", \"membership\", \"renewal\", \"plan\", \"ecommerce\", \"checkout\",\n",
    "            \"rent\", \"maintenance\", \"bill\", \"utility\", \"electricity\", \"water\", \"gas\", \"mobile\", \"internet\", \"broadband\", \"tv\", \"dth\", \"fastag\", \"toll\",\n",
    "            \"recharge\", \"topup\", \"cashback\", \"reward\", \"donation\", \"charity\", \"transfer to\", \"transfer from\",\n",
    "            \"upi-ref\", \"utr\", \"ref no\", \"txn id\", \"txn\"\n",
    "        ])\n",
    "\n",
    "        print(\"Detected Transaction Presence via Search:\", \"TRUE\" if has_transaction else \"FALSE\")\n",
    "\n",
    "        if not has_transaction:\n",
    "            enriched_transactions.append({\n",
    "                \"transaction_id\": idx + 1,\n",
    "                \"description\": description or \"Unknown Transaction\",\n",
    "                \"vendor\": vendor or \"Unknown Vendor\",\n",
    "                \"amount\": amount,\n",
    "                \"category\": \"Miscellaneous\",\n",
    "                \"confidence_score\": 0.5,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # --- Gemini Enrichment ---\n",
    "\n",
    "        # Infer description if missing\n",
    "        if not description:\n",
    "            prompt_desc = f\"\"\"\n",
    "            Given the vendor name: \"{vendor}\", and online information: \"{snippet_text}\",\n",
    "            generate a concise transaction description (3‚Äì5 words).\n",
    "            Only return the description. No other text.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                response = model.generate_content(prompt_desc)\n",
    "                description = response.text.strip()\n",
    "            except Exception:\n",
    "                description = f\"{vendor} Transaction\"\n",
    "\n",
    "        # Infer vendor if missing\n",
    "        if not vendor:\n",
    "            prompt_vendor = f\"\"\"\n",
    "            Based on this transaction description: \"{description}\" and search information: \"{snippet_text}\",\n",
    "            identify the most likely vendor or company name.\n",
    "            Return only the name. No explanation.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                response = model.generate_content(prompt_vendor)\n",
    "                vendor = response.text.strip()\n",
    "            except Exception:\n",
    "                vendor = \"Unknown Vendor\"\n",
    "\n",
    "        # Classify transaction with the generalized prompt and safe JSON parsing\n",
    "        prompt_category = f\"\"\"\n",
    "        {general_categorization_prompt}\n",
    "        \n",
    "        Please categorize this transaction:\n",
    "        Description: {description}\n",
    "        Vendor: {vendor}\n",
    "        Amount: ${amount}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = model.generate_content(prompt_category)\n",
    "            raw_response = response.text.strip()\n",
    "\n",
    "            # Extract JSON substring between first '{' and last '}'\n",
    "            json_start = raw_response.find(\"{\")\n",
    "            json_end = raw_response.rfind(\"}\")\n",
    "\n",
    "            if json_start != -1 and json_end != -1:\n",
    "                json_text = raw_response[json_start:json_end + 1]\n",
    "                classification = json.loads(json_text)\n",
    "            else:\n",
    "                classification = {}\n",
    "\n",
    "            category = classification.get(\"category\", \"Miscellaneous\")\n",
    "            confidence_score = classification.get(\"confidence\", 0.5)\n",
    "        except Exception as e:\n",
    "            print(f\"Gemini categorization error: {e}\")\n",
    "            print(f\"Raw response was: {raw_response if 'raw_response' in locals() else 'No response'}\")\n",
    "            category = \"Miscellaneous\"\n",
    "            confidence_score = 0.5\n",
    "\n",
    "        # If category is 'Merchandise', refine it based on vendor\n",
    "        if category.lower() == \"merchandise\":\n",
    "            prompt_vendor_category = f\"\"\"\n",
    "            The transaction was classified as 'Merchandise'.\n",
    "            Based on the vendor name: \"{vendor}\", suggest a more specific transaction category such as\n",
    "            Electronics, Apparel, Grocery, Books, or others.\n",
    "            \n",
    "            Respond only with the category name.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                response = model.generate_content(prompt_vendor_category)\n",
    "                vendor_category = response.text.strip()\n",
    "                if vendor_category:\n",
    "                    category = vendor_category\n",
    "            except Exception as e:\n",
    "                print(f\"Error refining category based on vendor: {e}\")\n",
    "\n",
    "        # Amazon check (optional)\n",
    "        prompt_amazon = f\"\"\"\n",
    "        Is this transaction related to Amazon or its subsidiaries (e.g., AWS, Whole Foods, Kindle, Twitch)?\n",
    "        \n",
    "        Description: {description}\n",
    "        Vendor: {vendor}\n",
    "        Amount: ${amount}\n",
    "        \n",
    "        Online context suggests Amazon reference: {\"amazon\" in snippet_text}\n",
    "        \n",
    "        Respond only with TRUE or FALSE\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = model.generate_content(prompt_amazon)\n",
    "            # Optionally capture or use response.text.strip() if needed\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Save results\n",
    "        enriched_transactions.append({\n",
    "            \"transaction_id\": idx + 1,\n",
    "            \"description\": description,\n",
    "            \"vendor\": vendor,\n",
    "            \"amount\": amount,\n",
    "            \"category\": category,\n",
    "            \"confidence_score\": confidence_score,\n",
    "        })\n",
    "\n",
    "        print(f\"Description: {description}\")\n",
    "        print(f\"Vendor: {vendor}\")\n",
    "        print(f\"Category: {category} | Confidence: {confidence_score}\")\n",
    "\n",
    "    # Optionally print all enriched transactions at the end\n",
    "    print(\"\\nAll enriched transactions:\")\n",
    "    for tx in enriched_transactions:\n",
    "        print(tx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import re\n",
    "# from typing import Dict, List, Tuple, Optional\n",
    "# import logging\n",
    "# from dataclasses import dataclass\n",
    "# import psycopg2\n",
    "# from psycopg2.extras import RealDictCursor\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# import sys\n",
    "# import google.generativeai as genai\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# @dataclass\n",
    "# class ClassificationResult:\n",
    "#     \"\"\"Data class for classification results\"\"\"\n",
    "#     classification_category: str\n",
    "#     reason: str\n",
    "#     confidence_score: float\n",
    "#     source: str\n",
    "\n",
    "# class TransactionClassifier:\n",
    "#     \"\"\"\n",
    "#     Advanced transaction classification system using RAG with master vectors and Gemini AI\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, model_name: str = 'all-MiniLM-L6-v2', similarity_threshold: float = 0.75,\n",
    "#                  database_url: str = None, gemini_api_key: str = None):\n",
    "#         \"\"\"\n",
    "#         Initialize the classifier\n",
    "        \n",
    "#         Args:\n",
    "#             model_name: Sentence transformer model name\n",
    "#             similarity_threshold: Minimum similarity score for master vector matching\n",
    "#             database_url: PostgreSQL connection URL\n",
    "#             gemini_api_key: Google Gemini API key\n",
    "#         \"\"\"\n",
    "#         self.model = SentenceTransformer(model_name)\n",
    "#         self.similarity_threshold = similarity_threshold\n",
    "#         self.master_vectors = None\n",
    "#         self.master_embeddings = None\n",
    "#         self.database_url = database_url\n",
    "#         self.use_database = False\n",
    "#         self.use_gemini = False\n",
    "        \n",
    "#         # Setup Gemini if API key provided\n",
    "#         if gemini_api_key:\n",
    "#             try:\n",
    "#                 genai.configure(api_key=gemini_api_key)\n",
    "#                 self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "#                 self.use_gemini = True\n",
    "#                 print(\"‚úÖ Gemini AI configured successfully\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"‚ö†Ô∏è  Gemini setup failed: {e}\")\n",
    "#                 self.use_gemini = False\n",
    "        \n",
    "#         # Standard categories for reference\n",
    "#         self.standard_categories = [\n",
    "#             'Food & Dining', 'Transportation', 'Shopping', 'Entertainment', \n",
    "#             'Utilities', 'Healthcare', 'Financial', 'Groceries', 'Income',\n",
    "#             'Travel', 'Education', 'Personal Care', 'Home & Garden', \n",
    "#             'Insurance', 'Taxes', 'Charity', 'Other'\n",
    "#         ]\n",
    "    \n",
    "#     def setup_database_connection(self):\n",
    "#         \"\"\"Interactive setup for database connection\"\"\"\n",
    "#         print(\"\\n\" + \"=\"*60)\n",
    "#         print(\"üîß DATABASE SETUP (PostgreSQL + pgvector)\")\n",
    "#         print(\"=\"*60)\n",
    "#         print(\"This classifier uses RAG with vector embeddings from PostgreSQL.\")\n",
    "#         print(\"Master vectors are REQUIRED for proper classification.\")\n",
    "        \n",
    "#         use_db = input(\"\\nDo you have PostgreSQL with master vectors set up? (y/n): \").strip().lower()\n",
    "        \n",
    "#         if use_db in ['y', 'yes']:\n",
    "#             print(\"\\nüìä Database Configuration:\")\n",
    "            \n",
    "#             # Get database connection details\n",
    "#             host = input(\"Host (default: localhost): \").strip() or 'localhost'\n",
    "#             port = input(\"Port (default: 5432): \").strip() or '5432'\n",
    "#             database = input(\"Database name (default: vector_db): \").strip() or 'vector_db'\n",
    "#             username = input(\"Username (default: vector_user): \").strip() or 'vector_user'\n",
    "#             password = input(\"Password (default: SecurePassword123!): \").strip() or 'SecurePassword123!'\n",
    "            \n",
    "#             self.database_url = f\"postgresql://{username}:{password}@{host}:{port}/{database}\"\n",
    "            \n",
    "#             # Test connection\n",
    "#             try:\n",
    "#                 print(\"\\nüîç Testing database connection...\")\n",
    "#                 conn = psycopg2.connect(self.database_url)\n",
    "#                 conn.close()\n",
    "#                 print(\"‚úÖ Database connection successful!\")\n",
    "#                 self.use_database = True\n",
    "#                 return True\n",
    "#             except Exception as e:\n",
    "#                 print(f\"‚ùå Database connection failed: {e}\")\n",
    "#                 print(\"‚ùå Cannot proceed without master vectors\")\n",
    "#                 return False\n",
    "#         else:\n",
    "#             print(\"‚ùå Master vectors are required for RAG-based classification\")\n",
    "#             print(\"üí° Please set up PostgreSQL with pgvector and master_vectors table\")\n",
    "#             return False\n",
    "    \n",
    "#     def setup_gemini_api(self):\n",
    "#         \"\"\"Interactive setup for Gemini API\"\"\"\n",
    "#         print(\"\\n\" + \"=\"*60)\n",
    "#         print(\"ü§ñ GEMINI AI SETUP (Optional)\")\n",
    "#         print(\"=\"*60)\n",
    "#         print(\"Gemini can help classify transactions that don't match master vectors.\")\n",
    "        \n",
    "#         use_gemini = input(\"\\nDo you want to use Gemini AI for fallback classification? (y/n): \").strip().lower()\n",
    "        \n",
    "#         if use_gemini in ['y', 'yes']:\n",
    "#             api_key = input(\"Enter your Gemini API key: \").strip()\n",
    "#             if api_key:\n",
    "#                 try:\n",
    "#                     genai.configure(api_key=api_key)\n",
    "#                     self.gemini_model = genai.GenerativeModel('gemini-pro')\n",
    "#                     self.use_gemini = True\n",
    "#                     print(\"‚úÖ Gemini AI configured successfully\")\n",
    "#                     return True\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"‚ùå Gemini setup failed: {e}\")\n",
    "#                     self.use_gemini = False\n",
    "#                     return False\n",
    "#             else:\n",
    "#                 print(\"‚ö†Ô∏è  No API key provided, skipping Gemini\")\n",
    "#                 return False\n",
    "#         else:\n",
    "#             print(\"üìù Skipping Gemini AI setup\")\n",
    "#             return False\n",
    "    \n",
    "#     def load_master_vectors_from_db(self, table_name: str = 'master_vectors') -> bool:\n",
    "#         \"\"\"\n",
    "#         Load master vector table from PostgreSQL database with pgvector\n",
    "#         \"\"\"\n",
    "#         if not self.use_database:\n",
    "#             return False\n",
    "            \n",
    "#         try:\n",
    "#             print(\"üîç Loading master vectors from database...\")\n",
    "            \n",
    "#             conn = psycopg2.connect(self.database_url)\n",
    "#             cursor = conn.cursor(cursor_factory=RealDictCursor)\n",
    "            \n",
    "#             # Query to fetch master vectors\n",
    "#             query = f\"\"\"\n",
    "#             SELECT id, embedding, category, entity_or_keyword, source \n",
    "#             FROM {table_name}\n",
    "#             ORDER BY id\n",
    "#             \"\"\"\n",
    "            \n",
    "#             cursor.execute(query)\n",
    "#             rows = cursor.fetchall()\n",
    "            \n",
    "#             if not rows:\n",
    "#                 print(f\"‚ùå No data found in table {table_name}\")\n",
    "#                 print(\"üí° You need to populate master_vectors table with embeddings\")\n",
    "#                 cursor.close()\n",
    "#                 conn.close()\n",
    "#                 return False\n",
    "            \n",
    "#             print(f\"üìä Found {len(rows)} master vectors in database\")\n",
    "            \n",
    "#             # Convert to DataFrame\n",
    "#             master_data = []\n",
    "#             embeddings_list = []\n",
    "            \n",
    "#             for row in rows:\n",
    "#                 master_data.append({\n",
    "#                     'id': row['id'],\n",
    "#                     'category': row['category'],\n",
    "#                     'entity_or_keyword': row['entity_or_keyword'],\n",
    "#                     'source': row['source']\n",
    "#                 })\n",
    "                \n",
    "#                 # Handle pgvector embedding format\n",
    "#                 embedding = row['embedding']\n",
    "#                 if isinstance(embedding, str):\n",
    "#                     embedding = embedding.strip('[]')\n",
    "#                     embedding_array = np.array([float(x.strip()) for x in embedding.split(',')])\n",
    "#                 elif isinstance(embedding, list):\n",
    "#                     embedding_array = np.array(embedding)\n",
    "#                 else:\n",
    "#                     embedding_array = np.array(embedding)\n",
    "                \n",
    "#                 embeddings_list.append(embedding_array)\n",
    "            \n",
    "#             self.master_vectors = pd.DataFrame(master_data)\n",
    "#             self.master_embeddings = np.array(embeddings_list)\n",
    "            \n",
    "#             cursor.close()\n",
    "#             conn.close()\n",
    "            \n",
    "#             print(\"‚úÖ Master vectors loaded successfully\")\n",
    "            \n",
    "#             # Show sample of loaded vectors\n",
    "#             print(f\"\\nüìã Sample master vectors:\")\n",
    "#             print(self.master_vectors[['category', 'entity_or_keyword']].head().to_string(index=False))\n",
    "            \n",
    "#             return True\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ùå Database error: {e}\")\n",
    "#             return False\n",
    "    \n",
    "#     def generate_embedding(self, text: str) -> np.ndarray:\n",
    "#         \"\"\"Generate embedding for input text\"\"\"\n",
    "#         if not text or pd.isna(text):\n",
    "#             text = \"\"\n",
    "#         return self.model.encode([str(text)])[0]\n",
    "    \n",
    "#     def find_best_master_match(self, transaction_embedding: np.ndarray, top_k: int = 3) -> Tuple[Optional[Dict], float, List[Dict]]:\n",
    "#         \"\"\"\n",
    "#         Find the best matching master vector using RAG approach\n",
    "        \n",
    "#         Args:\n",
    "#             transaction_embedding: Embedding vector for the transaction\n",
    "#             top_k: Number of top matches to return for context\n",
    "            \n",
    "#         Returns:\n",
    "#             Tuple of (best_match_dict, similarity_score, top_matches_list)\n",
    "#         \"\"\"\n",
    "#         if self.master_embeddings is None or len(self.master_embeddings) == 0:\n",
    "#             return None, 0.0, []\n",
    "        \n",
    "#         # Calculate cosine similarities\n",
    "#         similarities = cosine_similarity([transaction_embedding], self.master_embeddings)[0]\n",
    "        \n",
    "#         # Get top k matches\n",
    "#         top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "#         top_matches = []\n",
    "        \n",
    "#         for idx in top_indices:\n",
    "#             match_data = self.master_vectors.iloc[idx].to_dict()\n",
    "#             match_data['similarity'] = similarities[idx]\n",
    "#             top_matches.append(match_data)\n",
    "        \n",
    "#         # Best match\n",
    "#         best_idx = top_indices[0]\n",
    "#         best_score = similarities[best_idx]\n",
    "        \n",
    "#         if best_score >= self.similarity_threshold:\n",
    "#             best_match = self.master_vectors.iloc[best_idx].to_dict()\n",
    "#             return best_match, best_score, top_matches\n",
    "        \n",
    "#         return None, best_score, top_matches\n",
    "    \n",
    "#     def classify_with_gemini(self, description: str, vendor: str = None, similar_matches: List[Dict] = None) -> ClassificationResult:\n",
    "#         \"\"\"\n",
    "#         Use Gemini AI for intelligent classification with RAG context\n",
    "        \n",
    "#         Args:\n",
    "#             description: Transaction description\n",
    "#             vendor: Vendor name\n",
    "#             similar_matches: List of similar matches from vector search for context\n",
    "            \n",
    "#         Returns:\n",
    "#             ClassificationResult object\n",
    "#         \"\"\"\n",
    "#         if not self.use_gemini:\n",
    "#             return ClassificationResult(\n",
    "#                 classification_category=\"Other\",\n",
    "#                 reason=\"No classification method available\",\n",
    "#                 confidence_score=0.1,\n",
    "#                 source=\"unknown\"\n",
    "#             )\n",
    "        \n",
    "#         try:\n",
    "#             # Prepare context from similar matches\n",
    "#             context = \"\"\n",
    "#             if similar_matches:\n",
    "#                 context = \"Similar transactions from database:\\n\"\n",
    "#                 for i, match in enumerate(similar_matches[:3]):\n",
    "#                     context += f\"{i+1}. '{match['entity_or_keyword']}' ‚Üí {match['category']} (similarity: {match.get('similarity', 0):.3f})\\n\"\n",
    "            \n",
    "#             # Create prompt for Gemini\n",
    "#             prompt = f\"\"\"\n",
    "# You are an expert financial transaction classifier. Classify the following transaction into one of these categories:\n",
    "\n",
    "# {', '.join(self.standard_categories)}\n",
    "\n",
    "# Transaction Details:\n",
    "# - Description: {description or 'N/A'}\n",
    "# - Vendor: {vendor or 'N/A'}\n",
    "\n",
    "# {context}\n",
    "\n",
    "# Based on the transaction details and similar matches above, please:\n",
    "# 1. Choose the most appropriate category\n",
    "# 2. Provide a brief reason for your classification\n",
    "# 3. Give a confidence score (0.0 to 1.0)\n",
    "\n",
    "# Respond in this exact format:\n",
    "# Category: [category name]\n",
    "# Reason: [brief explanation]\n",
    "# Confidence: [0.0 to 1.0]\n",
    "# \"\"\"\n",
    "            \n",
    "#             response = self.gemini_model.generate_content(prompt)\n",
    "#             response_text = response.text.strip()\n",
    "            \n",
    "#             # Parse Gemini response\n",
    "#             lines = response_text.split('\\n')\n",
    "#             category = \"Other\"\n",
    "#             reason = \"Gemini classification\"\n",
    "#             confidence = 0.6\n",
    "            \n",
    "#             for line in lines:\n",
    "#                 line = line.strip()\n",
    "#                 if line.startswith('Category:'):\n",
    "#                     category = line.replace('Category:', '').strip()\n",
    "#                 elif line.startswith('Reason:'):\n",
    "#                     reason = line.replace('Reason:', '').strip()\n",
    "#                 elif line.startswith('Confidence:'):\n",
    "#                     try:\n",
    "#                         confidence = float(line.replace('Confidence:', '').strip())\n",
    "#                     except:\n",
    "#                         confidence = 0.6\n",
    "            \n",
    "#             # Validate category\n",
    "#             if category not in self.standard_categories:\n",
    "#                 category = \"Other\"\n",
    "            \n",
    "#             return ClassificationResult(\n",
    "#                 classification_category=category,\n",
    "#                 reason=reason,\n",
    "#                 confidence_score=round(confidence, 3),\n",
    "#                 source=\"gemini_ai\"\n",
    "#             )\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ö†Ô∏è  Gemini classification failed: {e}\")\n",
    "#             return ClassificationResult(\n",
    "#                 classification_category=\"Other\",\n",
    "#                 reason=f\"Gemini error: {str(e)[:50]}...\",\n",
    "#                 confidence_score=0.2,\n",
    "#                 source=\"gemini_error\"\n",
    "#             )\n",
    "    \n",
    "#     def classify_transaction(self, description: str, vendor: str = None) -> ClassificationResult:\n",
    "#         \"\"\"\n",
    "#         Classify a single transaction using RAG approach\n",
    "        \n",
    "#         Args:\n",
    "#             description: Transaction description\n",
    "#             vendor: Vendor name (optional)\n",
    "            \n",
    "#         Returns:\n",
    "#             ClassificationResult object\n",
    "#         \"\"\"\n",
    "#         transaction_text = f\"{description or ''} {vendor or ''}\".strip()\n",
    "#         if not transaction_text:\n",
    "#             return ClassificationResult(\n",
    "#                 classification_category=\"Other\",\n",
    "#                 reason=\"Empty transaction description\",\n",
    "#                 confidence_score=0.1,\n",
    "#                 source=\"empty\"\n",
    "#             )\n",
    "        \n",
    "#         # Generate embedding for RAG search\n",
    "#         transaction_embedding = self.generate_embedding(transaction_text)\n",
    "        \n",
    "#         # Try to match with master vectors (RAG approach)\n",
    "#         best_match, similarity_score, top_matches = self.find_best_master_match(transaction_embedding, top_k=5)\n",
    "        \n",
    "#         if best_match and similarity_score >= self.similarity_threshold:\n",
    "#             return ClassificationResult(\n",
    "#                 classification_category=best_match['category'],\n",
    "#                 reason=f\"RAG match: {best_match['entity_or_keyword']}\",\n",
    "#                 confidence_score=round(similarity_score, 3),\n",
    "#                 source=\"rag_vector\"\n",
    "#             )\n",
    "        \n",
    "#         # If no good vector match, use Gemini with RAG context\n",
    "#         if self.use_gemini:\n",
    "#             return self.classify_with_gemini(description, vendor, top_matches)\n",
    "        \n",
    "#         # If no Gemini, return low confidence classification\n",
    "#         return ClassificationResult(\n",
    "#             classification_category=\"Other\",\n",
    "#             reason=\"No similar vectors found, no AI available\",\n",
    "#             confidence_score=0.2,\n",
    "#             source=\"no_match\"\n",
    "#         )\n",
    "    \n",
    "#     def classify_transactions(self, transactions_df: pd.DataFrame) -> pd.DataFrame:\n",
    "#         \"\"\"Classify multiple transactions using RAG + AI approach\"\"\"\n",
    "#         print(f\"\\nü§ñ Classifying {len(transactions_df)} transactions using RAG approach...\")\n",
    "        \n",
    "#         # Find description column\n",
    "#         description_col = None\n",
    "#         vendor_col = None\n",
    "        \n",
    "#         # Check for description column\n",
    "#         for col in transactions_df.columns:\n",
    "#             col_lower = col.lower()\n",
    "#             if 'description' in col_lower or 'desc' in col_lower:\n",
    "#                 description_col = col\n",
    "#                 break\n",
    "        \n",
    "#         # Check for vendor column\n",
    "#         for col in transactions_df.columns:\n",
    "#             col_lower = col.lower()\n",
    "#             if 'vendor' in col_lower or 'merchant' in col_lower:\n",
    "#                 vendor_col = col\n",
    "#                 break\n",
    "        \n",
    "#         # If no description column found, use vendor or first text column\n",
    "#         if not description_col:\n",
    "#             if vendor_col:\n",
    "#                 description_col = vendor_col\n",
    "#             else:\n",
    "#                 for col in transactions_df.columns:\n",
    "#                     if transactions_df[col].dtype == 'object':\n",
    "#                         description_col = col\n",
    "#                         break\n",
    "        \n",
    "#         if not description_col:\n",
    "#             raise ValueError(\"No suitable text column found for transaction description\")\n",
    "        \n",
    "#         print(f\"üìù Using column '{description_col}' for transaction descriptions\")\n",
    "#         if vendor_col:\n",
    "#             print(f\"üè™ Using column '{vendor_col}' for vendor information\")\n",
    "        \n",
    "#         # Create a copy\n",
    "#         result_df = transactions_df.copy()\n",
    "        \n",
    "#         # Initialize classification columns\n",
    "#         result_df['classification_category'] = ''\n",
    "#         result_df['reason'] = ''\n",
    "#         result_df['confidence_score'] = 0.0\n",
    "#         result_df['source'] = ''\n",
    "        \n",
    "#         # Classify each transaction\n",
    "#         rag_matches = 0\n",
    "#         ai_matches = 0\n",
    "        \n",
    "#         for idx, row in result_df.iterrows():\n",
    "#             description = str(row[description_col]) if pd.notna(row[description_col]) else \"\"\n",
    "#             vendor = str(row[vendor_col]) if vendor_col and pd.notna(row.get(vendor_col, '')) else \"\"\n",
    "            \n",
    "#             classification = self.classify_transaction(description, vendor)\n",
    "            \n",
    "#             result_df.at[idx, 'classification_category'] = classification.classification_category\n",
    "#             result_df.at[idx, 'reason'] = classification.reason\n",
    "#             result_df.at[idx, 'confidence_score'] = classification.confidence_score\n",
    "#             result_df.at[idx, 'source'] = classification.source\n",
    "            \n",
    "#             # Count classification sources\n",
    "#             if classification.source == 'rag_vector':\n",
    "#                 rag_matches += 1\n",
    "#             elif classification.source == 'gemini_ai':\n",
    "#                 ai_matches += 1\n",
    "            \n",
    "#             # Show progress\n",
    "#             if (idx + 1) % 25 == 0 or idx == 0:\n",
    "#                 print(f\"üìä Processed {idx + 1}/{len(result_df)} transactions (RAG: {rag_matches}, AI: {ai_matches})\")\n",
    "        \n",
    "#         print(f\"‚úÖ Classification completed! (RAG matches: {rag_matches}, AI classifications: {ai_matches})\")\n",
    "#         return result_df\n",
    "    \n",
    "#     def get_classification_summary(self, classified_df: pd.DataFrame) -> Dict:\n",
    "#         \"\"\"Generate summary statistics\"\"\"\n",
    "#         summary = {\n",
    "#             'total_transactions': len(classified_df),\n",
    "#             'rag_vector_matches': len(classified_df[classified_df['source'] == 'rag_vector']),\n",
    "#             'gemini_ai_classifications': len(classified_df[classified_df['source'] == 'gemini_ai']),\n",
    "#             'no_match_count': len(classified_df[classified_df['source'].isin(['no_match', 'empty'])]),\n",
    "#             'average_confidence': round(classified_df['confidence_score'].mean(), 3),\n",
    "#             'category_distribution': classified_df['classification_category'].value_counts().to_dict(),\n",
    "#             'source_distribution': classified_df['source'].value_counts().to_dict(),\n",
    "#             'high_confidence_matches': len(classified_df[classified_df['confidence_score'] >= 0.8])\n",
    "#         }\n",
    "#         return summary\n",
    "\n",
    "# def get_user_inputs():\n",
    "#     \"\"\"Interactive function to get all user inputs\"\"\"\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"üéØ RAG-BASED TRANSACTION CLASSIFIER\")\n",
    "#     print(\"=\"*60)\n",
    "#     print(\"This tool uses RAG (Retrieval Augmented Generation) with vector embeddings\")\n",
    "#     print(\"and optional Gemini AI for intelligent transaction classification.\")\n",
    "    \n",
    "#     # Get input file\n",
    "#     print(\"\\nüìÅ INPUT FILE\")\n",
    "#     print(\"-\" * 20)\n",
    "#     while True:\n",
    "#         input_file = \"bank_transactions_with_vendor_100.csv\"#input(\"Enter path to your CSV file: \").strip().strip('\"\\'')\n",
    "#         if not input_file:\n",
    "#             print(\"‚ùå Please enter a file path\")\n",
    "#             continue\n",
    "#         if not os.path.exists(input_file):\n",
    "#             print(f\"‚ùå File not found: {input_file}\")\n",
    "#             continue\n",
    "#         if not input_file.lower().endswith('.csv'):\n",
    "#             print(\"‚ö†Ô∏è  Warning: File doesn't have .csv extension. Continue? (y/n): \", end=\"\")\n",
    "#             if input().strip().lower() not in ['y', 'yes']:\n",
    "#                 continue\n",
    "#         break\n",
    "    \n",
    "#     # Get output file\n",
    "#     print(\"\\nüíæ OUTPUT FILE\")\n",
    "#     print(\"-\" * 20)\n",
    "#     output_file ='out.csv' #input(\"Enter output file path (press Enter for auto-generated): \").strip().strip('\"\\'')\n",
    "#     if not output_file:\n",
    "#         base_name = os.path.splitext(input_file)[0]\n",
    "#         output_file = f\"{base_name}_rag_classified.csv\"\n",
    "#         print(f\"üìù Auto-generated output: {output_file}\")\n",
    "    \n",
    "#     # Get similarity threshold\n",
    "#     print(\"\\nüéØ SIMILARITY THRESHOLD\")\n",
    "#     print(\"-\" * 25)\n",
    "#     print(\"Higher values = stricter RAG matching (0.0 to 1.0)\")\n",
    "#     while True:\n",
    "#         threshold_input = input(\"Enter similarity threshold (press Enter for 0.75): \").strip()\n",
    "#         if not threshold_input:\n",
    "#             threshold = 0.75\n",
    "#             break\n",
    "#         try:\n",
    "#             threshold = float(threshold_input)\n",
    "#             if 0.0 <= threshold <= 1.0:\n",
    "#                 break\n",
    "#             else:\n",
    "#                 print(\"‚ùå Please enter a value between 0.0 and 1.0\")\n",
    "#         except ValueError:\n",
    "#             print(\"‚ùå Please enter a valid number\")\n",
    "    \n",
    "#     return input_file, output_file, threshold\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"Main interactive function\"\"\"\n",
    "#     try:\n",
    "#         # Get user inputs\n",
    "#         input_file, output_file, threshold = get_user_inputs()\n",
    "        \n",
    "#         # Initialize classifier\n",
    "#         print(\"\\nüöÄ INITIALIZING RAG CLASSIFIER\")\n",
    "#         print(\"-\" * 35)\n",
    "#         classifier = TransactionClassifier(similarity_threshold=threshold)\n",
    "        \n",
    "#         # Setup database connection (REQUIRED for RAG)\n",
    "#         if not classifier.setup_database_connection():\n",
    "#             print(\"‚ùå Cannot proceed without PostgreSQL database and master vectors\")\n",
    "#             return\n",
    "        \n",
    "#         # Setup Gemini API (OPTIONAL)\n",
    "#         classifier.setup_gemini_api()\n",
    "        \n",
    "#         # Load master vectors (REQUIRED)\n",
    "#         print(\"\\nüìä Loading master vectors for RAG...\")\n",
    "#         if not classifier.load_master_vectors_from_db():\n",
    "#             print(\"‚ùå Cannot proceed without master vectors\")\n",
    "#             return\n",
    "        \n",
    "#         # Read input file\n",
    "#         print(f\"\\nüìñ READING INPUT FILE\")\n",
    "#         print(\"-\" * 25)\n",
    "#         print(f\"Loading: {input_file}\")\n",
    "        \n",
    "#         try:\n",
    "#             transactions_df = pd.read_csv(input_file)\n",
    "#             print(f\"‚úÖ Loaded {len(transactions_df)} transactions\")\n",
    "            \n",
    "#             # Show sample data\n",
    "#             print(f\"\\nüìã Columns found: {list(transactions_df.columns)}\")\n",
    "#             print(\"\\nFirst 3 rows:\")\n",
    "#             print(transactions_df.head(3).to_string(index=False, max_cols=5))\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ùå Error reading CSV file: {e}\")\n",
    "#             return\n",
    "        \n",
    "#         # Classify transactions using RAG\n",
    "#         enriched_df = classifier.classify_transactions(transactions_df)\n",
    "        \n",
    "#         # Save output\n",
    "#         print(f\"\\nüíæ SAVING RESULTS\")\n",
    "#         print(\"-\" * 20)\n",
    "#         enriched_df.to_csv(output_file, index=False)\n",
    "#         print(f\"‚úÖ Results saved to: {output_file}\")\n",
    "        \n",
    "#         # Generate summary\n",
    "#         summary = classifier.get_classification_summary(enriched_df)\n",
    "        \n",
    "#         print(\"\\n\" + \"=\"*60)\n",
    "#         print(\"üìä RAG CLASSIFICATION SUMMARY\")\n",
    "#         print(\"=\"*60)\n",
    "#         print(f\"Total transactions: {summary['total_transactions']}\")\n",
    "#         print(f\"RAG vector matches: {summary['rag_vector_matches']}\")\n",
    "#         if summary['gemini_ai_classifications'] > 0:\n",
    "#             print(f\"Gemini AI classifications: {summary['gemini_ai_classifications']}\")\n",
    "#         print(f\"No match/empty: {summary['no_match_count']}\")\n",
    "#         print(f\"High confidence matches (‚â•0.8): {summary['high_confidence_matches']}\")\n",
    "#         print(f\"Average confidence: {summary['average_confidence']}\")\n",
    "        \n",
    "#         print(f\"\\nüìÇ Category Distribution:\")\n",
    "#         for category, count in summary['category_distribution'].items():\n",
    "#             percentage = (count / summary['total_transactions']) * 100\n",
    "#             print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "#         print(f\"\\nüîç Source Distribution:\")\n",
    "#         for source, count in summary['source_distribution'].items():\n",
    "#             percentage = (count / summary['total_transactions']) * 100\n",
    "#             print(f\"  {source}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "#         # Show sample results\n",
    "#         print(f\"\\nüìã Sample Results:\")\n",
    "#         display_columns = ['classification_category', 'confidence_score', 'source', 'reason']\n",
    "#         available_columns = [col for col in display_columns if col in enriched_df.columns]\n",
    "#         original_cols = list(transactions_df.columns)[:2]\n",
    "#         sample_columns = original_cols + available_columns\n",
    "        \n",
    "#         print(enriched_df[sample_columns].head(5).to_string(index=False))\n",
    "        \n",
    "#         print(f\"\\nüéâ RAG CLASSIFICATION SUCCESS!\")\n",
    "#         print(f\"üìÅ Output file: {output_file}\")\n",
    "        \n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"\\n\\n‚èπÔ∏è  Process interrupted by user\")\n",
    "#         sys.exit(0)\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå Error: {e}\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
